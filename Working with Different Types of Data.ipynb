{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Types\n",
    "\n",
    "In Spark, when we refer to \"Spark types,\" we are typically talking about the data types that are used to represent and process data within the Spark framework. Spark provides a set of built-in data types that can be used when working with DataFrames, Datasets, and other Spark-related APIs. Here are some of the common Spark types:\n",
    "\n",
    "1. **Numeric Types:**\n",
    "   - **`IntegerType`:** Represents 32-bit signed integers (`int` in Python).\n",
    "   - **`LongType`:** Represents 64-bit signed integers (`long` in Python).\n",
    "   - **`FloatType`:** Represents 32-bit floating-point numbers (`float` in Python).\n",
    "   - **`DoubleType`:** Represents 64-bit double-precision floating-point numbers (`double` in Python).\n",
    "\n",
    "2. **String Type:**\n",
    "   - **`StringType`:** Represents variable-length character strings (`str` in Python).\n",
    "\n",
    "3. **Boolean Type:**\n",
    "   - **`BooleanType`:** Represents boolean values (`bool` in Python).\n",
    "\n",
    "4. **Binary Type:**\n",
    "   - **`BinaryType`:** Represents binary data, typically used for storing raw binary blobs.\n",
    "\n",
    "5. **Timestamp and Date Types:**\n",
    "   - **`TimestampType`:** Represents a timestamp with both date and time information.\n",
    "   - **`DateType`:** Represents a date without time information.\n",
    "\n",
    "6. **ArrayType:**\n",
    "   - **`ArrayType`:** Represents an array or list of elements. Elements can be of any valid Spark type, including nested arrays.\n",
    "\n",
    "7. **MapType:**\n",
    "   - **`MapType`:** Represents a map or dictionary where keys and values can be of any valid Spark type.\n",
    "\n",
    "8. **StructType and StructField:**\n",
    "   - **`StructType`:** Represents a structure or a row with named fields.\n",
    "   - **`StructField`:** Represents a field within a `StructType`.\n",
    "\n",
    "9. **Decimal Type:**\n",
    "   - **`DecimalType`:** Represents arbitrary-precision decimals. It is often used for financial data.\n",
    "\n",
    "10. **User-Defined Types (UDTs):**\n",
    "    - Spark allows users to define custom data types by creating User-Defined Types (UDTs). This is useful when working with specialized data types that are not covered by the built-in types.\n",
    "\n",
    "These Spark types are used when defining the schema for DataFrames or Datasets, specifying the data types of columns, and ensuring type safety during data processing operations. When working with PySpark in Python, these types are often reflected in the Python equivalents (e.g., `IntegerType` corresponds to Python's `int`). Understanding and correctly using these types are essential for effective data processing and analysis with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Different Types of Data\n",
    "\n",
    " Let's go through examples of working with different types of data, including numbers, strings, booleans, and complex types, in PySpark DataFrames.\n",
    "\n",
    "### Working with Numbers:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Sample data with numbers\n",
    "data = [(1, 10.5), (2, 20.3), (3, 30.1)]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"id\", \"value\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_numbers = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_numbers.show()\n",
    "\n",
    "# Perform operations on numeric columns\n",
    "result_numbers = df_numbers.withColumn(\"double_value\", col(\"value\") * 2)\n",
    "\n",
    "# Show the result\n",
    "result_numbers.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "In this example, we create a DataFrame with numeric data and perform a simple operation, doubling the values in the \"value\" column.\n",
    "\n",
    "### Working with Strings:\n",
    "\n",
    "```python\n",
    "# Sample data with strings\n",
    "data_strings = [(\"Alice\", \"New York\"), (\"Bob\", \"San Francisco\"), (\"Charlie\", \"Los Angeles\")]\n",
    "\n",
    "# Define the schema\n",
    "schema_strings = [\"name\", \"city\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_strings = spark.createDataFrame(data_strings, schema=schema_strings)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_strings.show()\n",
    "\n",
    "# Concatenate string columns\n",
    "result_strings = df_strings.withColumn(\"full_location\", expr(\"name || ' in ' || city\"))\n",
    "\n",
    "# Show the result\n",
    "result_strings.show()\n",
    "```\n",
    "\n",
    "Here, we create a DataFrame with strings and concatenate the \"name\" and \"city\" columns to create a new column, \"full_location.\"\n",
    "\n",
    "### Working with Booleans:\n",
    "\n",
    "```python\n",
    "# Sample data with booleans\n",
    "data_booleans = [(\"Alice\", True), (\"Bob\", False), (\"Charlie\", True)]\n",
    "\n",
    "# Define the schema\n",
    "schema_booleans = [\"name\", \"is_student\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_booleans = spark.createDataFrame(data_booleans, schema=schema_booleans)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_booleans.show()\n",
    "\n",
    "# Filter rows based on boolean condition\n",
    "result_booleans = df_booleans.filter(col(\"is_student\") == True)\n",
    "\n",
    "# Show the result\n",
    "result_booleans.show()\n",
    "```\n",
    "\n",
    "In this example, we create a DataFrame with boolean values and filter rows based on a boolean condition.\n",
    "\n",
    "### Working with Complex Types:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Sample data with complex types\n",
    "data_complex = [(1, (\"Alice\", \"New York\")), (2, (\"Bob\", \"San Francisco\")), (3, (\"Charlie\", \"Los Angeles\"))]\n",
    "\n",
    "# Define the schema with a struct type\n",
    "schema_complex = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"info\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame\n",
    "df_complex = spark.createDataFrame(data_complex, schema=schema_complex)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_complex.show()\n",
    "\n",
    "# Access elements in a struct column\n",
    "result_complex = df_complex.withColumn(\"person_name\", col(\"info.name\"))\n",
    "\n",
    "# Show the result\n",
    "result_complex.show()\n",
    "```\n",
    "\n",
    "Certainly! Let's continue exploring more examples with different types of data in PySpark DataFrames.\n",
    "\n",
    "### Working with Arrays:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Sample data with arrays\n",
    "data_arrays = [(\"Alice\", \"New York,Chicago\"), (\"Bob\", \"San Francisco,Los Angeles\"), (\"Charlie\", \"Seattle\")]\n",
    "\n",
    "# Define the schema\n",
    "schema_arrays = [\"name\", \"locations\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_arrays = spark.createDataFrame(data_arrays, schema=schema_arrays)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_arrays.show()\n",
    "\n",
    "# Split the string in the \"locations\" column into an array\n",
    "result_arrays = df_arrays.withColumn(\"location_array\", split(col(\"locations\"), \",\"))\n",
    "\n",
    "# Show the result\n",
    "result_arrays.show()\n",
    "```\n",
    "\n",
    "In this example, we create a DataFrame with a string column containing comma-separated locations. We use the `split` function to split the string into an array.\n",
    "\n",
    "### Working with Maps:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# Sample data with maps\n",
    "data_maps = [(\"Alice\", \"NY\"), (\"Bob\", \"SF\"), (\"Charlie\", \"LA\")]\n",
    "\n",
    "# Define the schema\n",
    "schema_maps = [\"name\", \"abbreviation\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_maps = spark.createDataFrame(data_maps, schema=schema_maps)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_maps.show()\n",
    "\n",
    "# Create a map column\n",
    "result_maps = df_maps.withColumn(\"location_map\", create_map(col(\"name\"), col(\"abbreviation\")))\n",
    "\n",
    "# Show the result\n",
    "result_maps.show()\n",
    "```\n",
    "\n",
    "Here, we create a DataFrame with two columns, \"name\" and \"abbreviation.\" We use the `create_map` function to create a map column, mapping names to abbreviations.\n",
    "\n",
    "### Working with Dates and Timestamps:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "# Create a DataFrame with current date and timestamp\n",
    "df_dates = spark.createDataFrame([(1, current_date(), current_timestamp())], [\"id\", \"current_date\", \"current_timestamp\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df_dates.show()\n",
    "\n",
    "# Add 5 days to the current date\n",
    "result_dates = df_dates.withColumn(\"future_date\", col(\"current_date\") + 5)\n",
    "\n",
    "# Show the result\n",
    "result_dates.show()\n",
    "```\n",
    "\n",
    "In this example, we create a DataFrame with a column for the current date and another for the current timestamp. We then add 5 days to the current date using simple arithmetic.\n",
    "\n",
    "These examples cover a range of scenarios when working with different types of data in PySpark DataFrames. Remember to adjust these examples based on your specific use cases and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1| 10.5|\n",
      "|  2| 20.3|\n",
      "|  3| 30.1|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+------------+\n",
      "| id|value|double_value|\n",
      "+---+-----+------------+\n",
      "|  1| 10.5|        21.0|\n",
      "|  2| 20.3|        40.6|\n",
      "|  3| 30.1|        60.2|\n",
      "+---+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Sample data with numbers\n",
    "data = [(1, 10.5), (2, 20.3), (3, 30.1)]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"id\", \"value\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_numbers = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_numbers.show()\n",
    "\n",
    "# Perform operations on numeric columns\n",
    "result_numbers = df_numbers.withColumn(\"double_value\", col(\"value\") * 2)\n",
    "\n",
    "# Show the result\n",
    "result_numbers.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|   name|         city|\n",
      "+-------+-------------+\n",
      "|  Alice|     New York|\n",
      "|    Bob|San Francisco|\n",
      "|Charlie|  Los Angeles|\n",
      "+-------+-------------+\n",
      "\n",
      "+-------+-------------+--------------------+\n",
      "|   name|         city|       full_location|\n",
      "+-------+-------------+--------------------+\n",
      "|  Alice|     New York|   Alice in New York|\n",
      "|    Bob|San Francisco|Bob in San Francisco|\n",
      "|Charlie|  Los Angeles|Charlie in Los An...|\n",
      "+-------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data with strings\n",
    "data_strings = [(\"Alice\", \"New York\"), (\"Bob\", \"San Francisco\"), (\"Charlie\", \"Los Angeles\")]\n",
    "\n",
    "# Define the schema\n",
    "schema_strings = [\"name\", \"city\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_strings = spark.createDataFrame(data_strings, schema=schema_strings)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_strings.show()\n",
    "\n",
    "# Concatenate string columns\n",
    "result_strings = df_strings.withColumn(\"full_location\", expr(\"name || ' in ' || city\"))\n",
    "\n",
    "# Show the result\n",
    "result_strings.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   name|is_student|\n",
      "+-------+----------+\n",
      "|  Alice|      true|\n",
      "|    Bob|     false|\n",
      "|Charlie|      true|\n",
      "+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   name|is_student|\n",
      "+-------+----------+\n",
      "|  Alice|      true|\n",
      "|Charlie|      true|\n",
      "+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sample data with booleans\n",
    "data_booleans = [(\"Alice\", True), (\"Bob\", False), (\"Charlie\", True)]\n",
    "\n",
    "# Define the schema\n",
    "schema_booleans = [\"name\", \"is_student\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_booleans = spark.createDataFrame(data_booleans, schema=schema_booleans)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_booleans.show()\n",
    "\n",
    "# Filter rows based on boolean condition\n",
    "result_booleans = df_booleans.filter(col(\"is_student\") == True)\n",
    "\n",
    "# Show the result\n",
    "result_booleans.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                info|\n",
      "+---+--------------------+\n",
      "|  1|   {Alice, New York}|\n",
      "|  2|{Bob, San Francisco}|\n",
      "|  3|{Charlie, Los Ang...|\n",
      "+---+--------------------+\n",
      "\n",
      "+---+--------------------+-----------+\n",
      "| id|                info|person_name|\n",
      "+---+--------------------+-----------+\n",
      "|  1|   {Alice, New York}|      Alice|\n",
      "|  2|{Bob, San Francisco}|        Bob|\n",
      "|  3|{Charlie, Los Ang...|    Charlie|\n",
      "+---+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Sample data with complex types\n",
    "data_complex = [(1, (\"Alice\", \"New York\")), (2, (\"Bob\", \"San Francisco\")), (3, (\"Charlie\", \"Los Angeles\"))]\n",
    "\n",
    "# Define the schema with a struct type\n",
    "schema_complex = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"info\", StructType([\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame\n",
    "df_complex = spark.createDataFrame(data_complex, schema=schema_complex)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_complex.show()\n",
    "\n",
    "# Access elements in a struct column\n",
    "result_complex = df_complex.withColumn(\"person_name\", col(\"info.name\"))\n",
    "\n",
    "# Show the result\n",
    "result_complex.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|   name|           locations|\n",
      "+-------+--------------------+\n",
      "|  Alice|    New York,Chicago|\n",
      "|    Bob|San Francisco,Los...|\n",
      "|Charlie|             Seattle|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+--------------------+\n",
      "|   name|           locations|      location_array|\n",
      "+-------+--------------------+--------------------+\n",
      "|  Alice|    New York,Chicago| [New York, Chicago]|\n",
      "|    Bob|San Francisco,Los...|[San Francisco, L...|\n",
      "|Charlie|             Seattle|           [Seattle]|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Sample data with arrays\n",
    "data_arrays = [(\"Alice\", \"New York,Chicago\"), (\"Bob\", \"San Francisco,Los Angeles\"), (\"Charlie\", \"Seattle\")]\n",
    "\n",
    "# Define the schema\n",
    "schema_arrays = [\"name\", \"locations\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_arrays = spark.createDataFrame(data_arrays, schema=schema_arrays)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_arrays.show()\n",
    "\n",
    "# Split the string in the \"locations\" column into an array\n",
    "result_arrays = df_arrays.withColumn(\"location_array\", split(col(\"locations\"), \",\"))\n",
    "\n",
    "# Show the result\n",
    "result_arrays.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|   name|abbreviation|\n",
      "+-------+------------+\n",
      "|  Alice|          NY|\n",
      "|    Bob|          SF|\n",
      "|Charlie|          LA|\n",
      "+-------+------------+\n",
      "\n",
      "+-------+------------+---------------+\n",
      "|   name|abbreviation|   location_map|\n",
      "+-------+------------+---------------+\n",
      "|  Alice|          NY|  {Alice -> NY}|\n",
      "|    Bob|          SF|    {Bob -> SF}|\n",
      "|Charlie|          LA|{Charlie -> LA}|\n",
      "+-------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# Sample data with maps\n",
    "data_maps = [(\"Alice\", \"NY\"), (\"Bob\", \"SF\"), (\"Charlie\", \"LA\")]\n",
    "\n",
    "# Define the schema\n",
    "schema_maps = [\"name\", \"abbreviation\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_maps = spark.createDataFrame(data_maps, schema=schema_maps)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_maps.show()\n",
    "\n",
    "# Create a map column\n",
    "result_maps = df_maps.withColumn(\"location_map\", create_map(col(\"name\"), col(\"abbreviation\")))\n",
    "\n",
    "# Show the result\n",
    "result_maps.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "# Create a DataFrame with current date and timestamp\n",
    "df_dates = spark.createDataFrame([(1, current_date(), current_timestamp())], [\"id\", \"current_date\", \"current_timestamp\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df_dates.show()\n",
    "\n",
    "# Add 5 days to the current date\n",
    "result_dates = df_dates.withColumn(\"future_date\", col(\"current_date\") + 5)\n",
    "\n",
    "# Show the result\n",
    "result_dates.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
