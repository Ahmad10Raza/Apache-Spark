{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Joins**\n",
    "\n",
    "In Spark, a join operation is used to combine two DataFrames based on a common column or set of columns. The result of a join is a new DataFrame that combines rows from the input DataFrames according to the specified conditions. Join operations are fundamental in data processing, allowing you to combine information from multiple sources.\n",
    "\n",
    "There are several types of joins in Spark, and the choice of join type depends on the desired result and the characteristics of the data. Here are the common join types in Spark:\n",
    "\n",
    "### 1. Inner Join:\n",
    "\n",
    "An inner join returns only the rows that have matching values in both DataFrames based on the specified condition.\n",
    "\n",
    "```python\n",
    "# Example of an inner join\n",
    "result_inner_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"inner\")\n",
    "```\n",
    "\n",
    "### 2. Left Outer Join (Left Join):\n",
    "\n",
    "A left outer join returns all the rows from the left DataFrame and the matching rows from the right DataFrame. If there is no match, the result will contain null values for columns from the right DataFrame.\n",
    "\n",
    "```python\n",
    "# Example of a left outer join\n",
    "result_left_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"left_outer\")\n",
    "```\n",
    "\n",
    "### 3. Right Outer Join (Right Join):\n",
    "\n",
    "A right outer join returns all the rows from the right DataFrame and the matching rows from the left DataFrame. If there is no match, the result will contain null values for columns from the left DataFrame.\n",
    "\n",
    "```python\n",
    "# Example of a right outer join\n",
    "result_right_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"right_outer\")\n",
    "```\n",
    "\n",
    "### 4. Full Outer Join (Full Join):\n",
    "\n",
    "A full outer join returns all the rows when there is a match in either the left or right DataFrame. If there is no match, the result will contain null values for columns from the DataFrame without a match.\n",
    "\n",
    "```python\n",
    "# Example of a full outer join\n",
    "result_full_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"outer\")\n",
    "```\n",
    "\n",
    "### 5. Left Semi Join:\n",
    "\n",
    "A left semi join returns all the rows from the left DataFrame where there is a match in the right DataFrame. It does not include the actual data from the right DataFrame, only the matching keys.\n",
    "\n",
    "```python\n",
    "# Example of a left semi join\n",
    "result_left_semi_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"left_semi\")\n",
    "```\n",
    "\n",
    "### 6. Left Anti Join:\n",
    "\n",
    "A left anti join returns all the rows from the left DataFrame where there is no match in the right DataFrame.\n",
    "\n",
    "```python\n",
    "# Example of a left anti join\n",
    "result_left_anti_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"left_anti\")\n",
    "```\n",
    "\n",
    "### 7. Cross Join (Cartesian Join):\n",
    "\n",
    "A cross join returns the Cartesian product of rows from both DataFrames, meaning each row from the left DataFrame is combined with every row from the right DataFrame.\n",
    "\n",
    "```python\n",
    "# Example of a cross join\n",
    "result_cross_join = df1.join(df2, \"common_column\")\n",
    "```\n",
    "\n",
    "Certainly! Let's continue with more explanations and examples:\n",
    "\n",
    "### 8. Cross Join (Cartesian Join):\n",
    "\n",
    "A cross join returns the Cartesian product of rows from both DataFrames, meaning each row from the left DataFrame is combined with every row from the right DataFrame.\n",
    "\n",
    "```python\n",
    "# Example of a cross join\n",
    "result_cross_join = df1.crossJoin(df2)\n",
    "```\n",
    "\n",
    "In this example, the `crossJoin` method is used to perform a cross join between `df1` and `df2`. This operation is often avoided due to its potential to produce a large result, especially on datasets with many rows.\n",
    "\n",
    "### 9. Natural Join:\n",
    "\n",
    "A natural join performs a join based on all columns with the same name in both DataFrames. It automatically identifies and joins columns with the same name.\n",
    "\n",
    "```python\n",
    "# Example of a natural join\n",
    "result_natural_join = df1.join(df2, \"common_column\")\n",
    "```\n",
    "\n",
    "In this example, the `common_column` is assumed to be present in both DataFrames, and the natural join is performed based on that common column.\n",
    "\n",
    "### 10. Cross Join with Conditions:\n",
    "\n",
    "You can perform a cross join with additional conditions using the `filter` method.\n",
    "\n",
    "```python\n",
    "# Example of a cross join with conditions\n",
    "result_cross_with_condition = df1.crossJoin(df2).filter(df1[\"common_column\"] == df2[\"common_column\"])\n",
    "```\n",
    "\n",
    "In this example, a cross join is first performed, and then a filter condition is applied to select only the rows where the common column matches.\n",
    "\n",
    "### 11. Join Using SQL Syntax:\n",
    "\n",
    "Spark also allows you to perform joins using SQL syntax by registering DataFrames as temporary SQL tables.\n",
    "\n",
    "```python\n",
    "# Register DataFrames as SQL tables\n",
    "df1.createOrReplaceTempView(\"table1\")\n",
    "df2.createOrReplaceTempView(\"table2\")\n",
    "\n",
    "# Perform a join using SQL syntax\n",
    "result_sql_join = spark.sql(\"SELECT * FROM table1 INNER JOIN table2 ON table1.common_column = table2.common_column\")\n",
    "```\n",
    "\n",
    "This example demonstrates how to register DataFrames as temporary SQL tables and perform a join using SQL syntax.\n",
    "\n",
    "### 12. Handling Duplicate Column Names:\n",
    "\n",
    "When joining DataFrames with duplicate column names, you can provide a list of columns for the join condition.\n",
    "\n",
    "```python\n",
    "# Example of joining with duplicate column names\n",
    "result_duplicate_columns = df1.join(df2, [\"common_column\", \"other_column\"])\n",
    "```\n",
    "\n",
    "Here, the join condition is specified explicitly to avoid ambiguity in case of duplicate column names.\n",
    "\n",
    "These additional examples and explanations provide a more comprehensive understanding of join operations in Spark. Choose the appropriate join type based on your specific use case and the structure of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is This Join Operation Similar To SQL Join?\n",
    "\n",
    "Yes, the join operations in Spark, as demonstrated in the examples, are quite similar to SQL join operations. Spark's DataFrame API borrows concepts and syntax from SQL, making it intuitive for users familiar with relational databases and SQL.\n",
    "\n",
    "Here's a comparison of Spark DataFrame join operations with their SQL counterparts:\n",
    "\n",
    "### 1. Inner Join:\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_inner_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"inner\")\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 INNER JOIN df2 ON df1.common_column = df2.common_column\n",
    "```\n",
    "\n",
    "### 2. Left Outer Join (Left Join):\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_left_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"left_outer\")\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 LEFT OUTER JOIN df2 ON df1.common_column = df2.common_column\n",
    "```\n",
    "\n",
    "### 3. Right Outer Join (Right Join):\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_right_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"right_outer\")\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 RIGHT OUTER JOIN df2 ON df1.common_column = df2.common_column\n",
    "```\n",
    "\n",
    "### 4. Full Outer Join (Full Join):\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_full_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"outer\")\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 FULL OUTER JOIN df2 ON df1.common_column = df2.common_column\n",
    "```\n",
    "\n",
    "### 5. Left Semi Join:\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_left_semi_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"left_semi\")\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 WHERE common_column IN (SELECT common_column FROM df2)\n",
    "```\n",
    "\n",
    "### 6. Left Anti Join:\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_left_anti_join = df1.join(df2, df1[\"common_column\"] == df2[\"common_column\"], \"left_anti\")\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 WHERE common_column NOT IN (SELECT common_column FROM df2)\n",
    "```\n",
    "\n",
    "### 7. Cross Join (Cartesian Join):\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_cross_join = df1.crossJoin(df2)\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 CROSS JOIN df2\n",
    "```\n",
    "\n",
    "### 8. Natural Join:\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_natural_join = df1.join(df2, \"common_column\")\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 NATURAL JOIN df2\n",
    "```\n",
    "\n",
    "### 9. Cross Join with Conditions:\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_cross_with_condition = df1.crossJoin(df2).filter(df1[\"common_column\"] == df2[\"common_column\"])\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 CROSS JOIN df2 WHERE df1.common_column = df2.common_column\n",
    "```\n",
    "\n",
    "### 10. Join Using SQL Syntax:\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_sql_join = spark.sql(\"SELECT * FROM df1 INNER JOIN df2 ON df1.common_column = df2.common_column\")\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 INNER JOIN df2 ON df1.common_column = df2.common_column\n",
    "```\n",
    "\n",
    "### 11. Handling Duplicate Column Names:\n",
    "\n",
    "**Spark:**\n",
    "```python\n",
    "result_duplicate_columns = df1.join(df2, [\"common_column\", \"other_column\"])\n",
    "```\n",
    "\n",
    "**SQL:**\n",
    "```sql\n",
    "SELECT * FROM df1 INNER JOIN df2 ON df1.common_column = df2.common_column AND df1.other_column = df2.other_column\n",
    "```\n",
    "\n",
    "In general, the syntax and semantics of Spark's DataFrame API closely align with SQL, making it convenient for users to transition from SQL to Spark for data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark=SparkSession.builder.appName('Joins').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "(0, \"Bill Chambers\", 0, [100]),\n",
    "(1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "(2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    ".toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "graduateProgram = spark.createDataFrame([\n",
    "(0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    ".toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "sparkStatus = spark.createDataFrame([\n",
    "(500, \"Vice President\"),\n",
    "(250, \"PMC Member\"),\n",
    "(100, \"Contributor\")])\\\n",
    ".toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n",
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n",
      "+---+--------------+\n",
      "| id|        status|\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|    PMC Member|\n",
      "|100|   Contributor|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()\n",
    "graduateProgram.show()\n",
    "sparkStatus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Joins\n",
    "Inner joins evaluate the keys in both of the DataFrames or tables and include (and join together)\n",
    "only the rows that evaluate to true. In the following example, we join the graduateProgram\n",
    "DataFrame with the person DataFrame to create a new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(graduate_program = id)'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "joinExpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joinType = \"inner\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Joins\n",
    "Outer joins evaluate the keys in both of the DataFrames or tables and includes (and joins\n",
    "together) the rows that evaluate to true or false. If there is no equivalent row in either the left or\n",
    "right DataFrame, Spark will insert null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|NULL|            NULL|            NULL|           NULL|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joinType = \"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All other Join Operation same as example given above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
