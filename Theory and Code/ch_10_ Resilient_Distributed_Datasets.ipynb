{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Resilient Distributed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Are the Low-Level APIs?\n",
    "\n",
    "In Apache Spark, the low-level APIs refer to the foundational programming interfaces that enable developers to interact directly with distributed data and control the execution of tasks in a Spark application. The two main low-level APIs in Spark are:\n",
    "\n",
    "1. **Resilient Distributed Datasets (RDD):**\n",
    "   - RDD is the fundamental data structure in Spark, representing a fault-tolerant collection of elements that can be processed in parallel. RDDs are immutable, distributed collections of objects that can be processed in parallel. RDDs can be created from external data sources or by transforming other RDDs through operations like `map`, `filter`, and `reduce`.\n",
    "\n",
    "   **Example: Creating and Transforming RDDs:**\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   # Create a SparkContext\n",
    "   sc = SparkContext(\"local\", \"RDDExample\")\n",
    "\n",
    "   # Create an RDD from a list\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Transformations on RDD\n",
    "   squared_rdd = rdd.map(lambda x: x**2)\n",
    "\n",
    "   # Actions on RDD\n",
    "   result = squared_rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "   print(result)\n",
    "   ```\n",
    "\n",
    "2. **Spark Core API:**\n",
    "   - Spark Core is the foundation of the Spark ecosystem and provides the basic functionality for distributed computing. It includes the essential components such as task scheduling, memory management, and fault recovery. Spark applications use the Spark Core API to interact with the underlying Spark engine.\n",
    "\n",
    "   **Example: Using Spark Core for Word Count:**\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   # Create a SparkContext\n",
    "   sc = SparkContext(\"local\", \"SparkCoreExample\")\n",
    "\n",
    "   # Read a text file and perform word count\n",
    "   text_file = sc.textFile(\"path/to/your/text/file.txt\")\n",
    "   word_count = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "   # Collect and print the results\n",
    "   results = word_count.collect()\n",
    "   for result in results:\n",
    "       print(result)\n",
    "   ```\n",
    "\n",
    "While RDD and Spark Core provide powerful capabilities for distributed computing, higher-level abstractions like DataFrames and Spark SQL have been introduced to simplify development for common use cases and improve optimization. In modern Spark applications, developers often use higher-level APIs for ease of use and productivity, but understanding the low-level APIs can be beneficial for fine-grained control and optimization in specific scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Use the Low-Level APIs?\n",
    "You should generally use the lower-level APIs in three situations:\n",
    "You need some functionality that you cannot find in the higher-level APIs; for example,\n",
    "1. if you need very tight control over physical data placement across the cluster.\n",
    "2. You need to maintain some legacy codebase written using RDDs.\n",
    "3. You need to do some custom shared variable manipulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "Resilient Distributed Datasets (RDDs) are a fundamental data structure in Apache Spark, serving as the building block for distributed and fault-tolerant data processing. RDDs provide a distributed collection of objects that can be processed in parallel across a cluster. They are designed to be fault-tolerant, meaning they can recover from node failures during computation.\n",
    "\n",
    "Here are key characteristics and concepts related to RDDs:\n",
    "\n",
    "1. **Immutable Distributed Collection:**\n",
    "   - RDDs are immutable, meaning their content cannot be changed once created. However, transformations on RDDs result in new RDDs. This immutability simplifies fault recovery and parallel processing.\n",
    "\n",
    "2. **Resilience:**\n",
    "   - RDDs are resilient to node failures. If a partition of an RDD is lost due to a node failure, Spark can recover the lost data by recomputing the lost partition from the original data and lineage information (information about the sequence of transformations).\n",
    "\n",
    "3. **Partitioning:**\n",
    "   - RDDs are divided into partitions, which are the basic units of parallelism. Each partition can be processed on a separate node in the Spark cluster. The number of partitions can be configured to control parallelism.\n",
    "\n",
    "4. **Transformation and Action:**\n",
    "   - RDDs support two types of operations: transformations and actions. Transformations create a new RDD from an existing one (e.g., `map`, `filter`), while actions return a value to the driver program or write data to an external storage system (e.g., `reduce`, `collect`).\n",
    "\n",
    "5. **Lazy Evaluation:**\n",
    "   - RDDs use lazy evaluation, meaning transformations are not executed immediately. Instead, they are evaluated only when an action is called. This optimization allows Spark to optimize the execution plan based on the entire sequence of transformations.\n",
    "\n",
    "6. **Data Lineage:**\n",
    "   - RDDs keep track of their lineage, which is a record of the sequence of transformations used to build the RDD. This lineage information is crucial for fault recovery. If a partition is lost, Spark can recompute it using the original data and the lineage.\n",
    "\n",
    "7. **Wide vs. Narrow Transformations:**\n",
    "   - Transformations are categorized as either narrow or wide. Narrow transformations (e.g., `map`, `filter`) do not require shuffling of data between partitions, while wide transformations (e.g., `groupByKey`, `reduceByKey`) require data shuffling, which can be more expensive.\n",
    "\n",
    "8. **Caching:**\n",
    "   - RDDs can be cached in memory to improve the performance of iterative algorithms or when the same dataset is used multiple times. Caching allows Spark to keep the data in memory across multiple stages of a computation.\n",
    "\n",
    "Here's a simple example of using RDDs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDDExample\")\n",
    "\n",
    "# Create an RDD from a list\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Transformations\n",
    "squared_rdd = rdd.map(lambda x: x**2)\n",
    "\n",
    "# Action\n",
    "result = squared_rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types Of RDD\n",
    "\n",
    " RDDs in Apache Spark can be broadly categorized into two types based on their content and behavior: \"generic\" RDDs and key-value RDDs.\n",
    "\n",
    "1. **Generic RDDs:**\n",
    "   - A generic RDD, often referred to as a \"simple\" or \"non-pair\" RDD, is a distributed collection of elements without any inherent key-value structure. Each element in the RDD is treated as an independent unit of data, and transformations and actions are applied to the entire dataset.\n",
    "\n",
    "   **Example:**\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   # Create a SparkContext\n",
    "   sc = SparkContext(\"local\", \"GenericRDDExample\")\n",
    "\n",
    "   # Create a generic RDD\n",
    "   data = [1, 2, 3, 4, 5]\n",
    "   rdd = sc.parallelize(data)\n",
    "\n",
    "   # Transformation and Action on a generic RDD\n",
    "   squared_rdd = rdd.map(lambda x: x**2)\n",
    "   result = squared_rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "   print(result)\n",
    "   ```\n",
    "\n",
    "2. **Key-Value RDDs:**\n",
    "   - Key-value RDDs, also known as \"pair\" RDDs, represent data as key-value pairs. Each element in the RDD is a tuple (key, value), where both key and value can be of any data type. Key-value RDDs are particularly useful for operations that involve grouping or aggregating data based on keys.\n",
    "\n",
    "   **Example:**\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   # Create a SparkContext\n",
    "   sc = SparkContext(\"local\", \"KeyValueRDDExample\")\n",
    "\n",
    "   # Create a key-value RDD\n",
    "   data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)]\n",
    "   kv_rdd = sc.parallelize(data)\n",
    "\n",
    "   # Transformation and Action on a key-value RDD\n",
    "   age_sum_by_name = kv_rdd.reduceByKey(lambda x, y: x + y)\n",
    "   age_sum_by_name.collect()\n",
    "   ```\n",
    "\n",
    "In the second example, the key-value RDD is created with tuples representing (name, age) pairs. The `reduceByKey` transformation is then used to calculate the sum of ages for each unique name.\n",
    "\n",
    "The choice between using a generic RDD or a key-value RDD depends on the nature of the data and the operations you intend to perform. Key-value RDDs are especially beneficial for certain types of operations, such as grouping by key, reducing by key, and joining with other key-value RDDs. They provide a convenient way to express relationships and dependencies in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations On RDD\n",
    "\n",
    "Transformations in Apache Spark are operations on RDDs that create a new RDD by applying a function to each element of the existing RDD. Transformations are lazy, meaning they are not executed immediately. Instead, they build a logical execution plan that is executed only when an action is called. Here are some common transformations in Spark:\n",
    "\n",
    "1. **`map(func)`**\n",
    "   - Applies a function to each element of the RDD and returns a new RDD of the results.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"MapTransformationExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Square each element using map transformation\n",
    "   squared_rdd = rdd.map(lambda x: x**2)\n",
    "   ```\n",
    "\n",
    "2. **`filter(func)`**\n",
    "   - Returns a new RDD containing only the elements that satisfy the given predicate.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"FilterTransformationExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Filter even numbers using filter transformation\n",
    "   filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "   ```\n",
    "\n",
    "3. **`flatMap(func)`**\n",
    "   - Similar to `map`, but each input item can be mapped to zero or more output items.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"FlatMapTransformationExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Duplicate each element using flatMap transformation\n",
    "   duplicated_rdd = rdd.flatMap(lambda x: [x, x])\n",
    "   ```\n",
    "\n",
    "4. **`union(other)`**\n",
    "   - Returns a new RDD that contains the elements of the source RDD and the other RDD.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"UnionTransformationExample\")\n",
    "   rdd1 = sc.parallelize([1, 2, 3])\n",
    "   rdd2 = sc.parallelize([3, 4, 5])\n",
    "\n",
    "   # Combine two RDDs using union transformation\n",
    "   combined_rdd = rdd1.union(rdd2)\n",
    "   ```\n",
    "\n",
    "5. **`groupByKey()`**\n",
    "   - Groups the elements of the RDD by key and returns a new RDD of `(key, values)` pairs.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"GroupByKeyTransformationExample\")\n",
    "   kv_rdd = sc.parallelize([(\"Alice\", 25), (\"Bob\", 30), (\"Alice\", 22)])\n",
    "\n",
    "   # Group elements by key using groupByKey transformation\n",
    "   grouped_rdd = kv_rdd.groupByKey()\n",
    "   ```\n",
    "\n",
    "6. **`reduceByKey(func)`**\n",
    "   - Groups the elements of the RDD by key and applies a reduce function to the values associated with each key.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"ReduceByKeyTransformationExample\")\n",
    "   kv_rdd = sc.parallelize([(\"Alice\", 25), (\"Bob\", 30), (\"Alice\", 22)])\n",
    "\n",
    "   # Sum ages by key using reduceByKey transformation\n",
    "   sum_by_key_rdd = kv_rdd.reduceByKey(lambda x, y: x + y)\n",
    "   ```\n",
    "\n",
    "These are just a few examples of transformation operations in Spark. Transformations are building blocks that allow you to express complex data manipulations and transformations on your RDDs. Remember that transformations are evaluated lazily, and their execution is triggered when an action is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions On RDD\n",
    "\n",
    "Actions in Apache Spark are operations that return a value to the driver program or write data to an external storage system. Unlike transformations, actions trigger the execution of the computation plan built by transformations. Here are some common actions in Spark:\n",
    "\n",
    "1. **`collect()`**\n",
    "   - Returns all the elements of the RDD as an array to the driver program. Be cautious when using `collect()` on large datasets, as it brings all the data to the driver, and it might not fit in memory.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"CollectActionExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Collect all elements to the driver program\n",
    "   result = rdd.collect()\n",
    "   ```\n",
    "\n",
    "2. **`count()`**\n",
    "   - Returns the number of elements in the RDD.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"CountActionExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Count the number of elements in the RDD\n",
    "   count = rdd.count()\n",
    "   ```\n",
    "\n",
    "3. **`first()`**\n",
    "   - Returns the first element of the RDD.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"FirstActionExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Get the first element of the RDD\n",
    "   first_element = rdd.first()\n",
    "   ```\n",
    "\n",
    "4. **`take(n)`**\n",
    "   - Returns the first `n` elements of the RDD.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"TakeActionExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Get the first three elements of the RDD\n",
    "   first_three_elements = rdd.take(3)\n",
    "   ```\n",
    "\n",
    "5. **`reduce(func)`**\n",
    "   - Aggregates the elements of the RDD using a specified reduce function. The function should be associative and commutative.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"ReduceActionExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Sum all elements using reduce action\n",
    "   sum_result = rdd.reduce(lambda x, y: x + y)\n",
    "   ```\n",
    "\n",
    "6. **`foreach(func)`**\n",
    "   - Applies a function to each element of the RDD. This is often used for side-effect operations like printing.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"ForeachActionExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Print each element using foreach action\n",
    "   rdd.foreach(lambda x: print(x))\n",
    "   ```\n",
    "\n",
    "7. **`saveAsTextFile(path)`**\n",
    "   - Writes the elements of the RDD as text files in the specified path.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"SaveAsTextFileActionExample\")\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "   # Save the RDD as a text file\n",
    "   rdd.saveAsTextFile(\"path/to/save/text/files\")\n",
    "   ```\n",
    "\n",
    "These actions trigger the actual computation of the RDD and return values to the driver program or write data to external storage. It's important to note that actions are the operations that lead to the execution of the entire Spark computation plan built by transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Context..\n",
    "\n",
    "In Apache Spark, a `SparkContext` is a fundamental entry point to interact with a Spark cluster. It represents the connection to a Spark cluster and is responsible for coordinating the execution of distributed Spark applications. The `SparkContext` is typically created once in a Spark application and serves as a central coordinator for the entire application.\n",
    "\n",
    "Here are key aspects and functionalities associated with the `SparkContext`:\n",
    "\n",
    "1. **Creation:**\n",
    "   - The `SparkContext` is created when a Spark application is initiated. It connects to the Spark cluster and coordinates the distribution of tasks.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   # Create a SparkContext\n",
    "   sc = SparkContext(\"local\", \"MySparkApplication\")\n",
    "   ```\n",
    "\n",
    "   In this example, the `SparkContext` is created with the `\"local\"` master, indicating that the Spark application runs in local mode on a single machine. In a real cluster, you would replace `\"local\"` with the address of your Spark cluster's master node.\n",
    "\n",
    "2. **Configuration:**\n",
    "   - The `SparkContext` allows you to configure various aspects of your Spark application, such as the application name, cluster configuration, and logging settings.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkConf, SparkContext\n",
    "\n",
    "   # Configure Spark\n",
    "   conf = SparkConf().setAppName(\"MySparkApplication\").setMaster(\"local\")\n",
    "   sc = SparkContext(conf=conf)\n",
    "   ```\n",
    "\n",
    "3. **Access to Cluster Resources:**\n",
    "   - The `SparkContext` manages the allocation of resources in the Spark cluster, including memory and CPU. It communicates with the cluster manager (e.g., Apache Mesos, Apache YARN, or Spark's standalone cluster manager) to request resources for running tasks.\n",
    "\n",
    "4. **Creation of RDDs:**\n",
    "   - RDDs (Resilient Distributed Datasets) are the fundamental data structure in Spark. The `SparkContext` is used to create RDDs from external data sources or by parallelizing existing data structures.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"RDDCreationExample\")\n",
    "\n",
    "   # Create an RDD from a list\n",
    "   rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "   ```\n",
    "\n",
    "5. **Coordinate Job Execution:**\n",
    "   - The `SparkContext` coordinates the execution of Spark jobs. It divides the application into stages and tasks, schedules tasks on worker nodes, and monitors their progress.\n",
    "\n",
    "6. **Control of Logging:**\n",
    "   - The `SparkContext` provides control over the logging level and other logging configurations for the Spark application.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(\"local\", \"LoggingExample\")\n",
    "\n",
    "   # Set the log level to ERROR\n",
    "   sc.setLogLevel(\"ERROR\")\n",
    "   ```\n",
    "\n",
    "7. **Stop the SparkContext:**\n",
    "   - Once the Spark application is complete, it's essential to stop the `SparkContext` to release resources and shut down the Spark cluster connections.\n",
    "\n",
    "   ```python\n",
    "   sc.stop()\n",
    "   ```\n",
    "\n",
    "The `SparkContext` is a crucial component for managing the execution and resources of Spark applications. In modern Spark applications, you might also encounter the use of `SparkSession`, which is a higher-level abstraction built on top of `SparkContext` and includes additional functionalities for working with DataFrames and Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Context V/s Spark Session\n",
    "\n",
    "`SparkContext` and `SparkSession` are both important components in Apache Spark, but they serve different purposes and have distinct roles in Spark applications.\n",
    "\n",
    "### SparkContext:\n",
    "\n",
    "1. **Role:**\n",
    "   - `SparkContext` is the entry point and the central coordinator for low-level Spark functionality.\n",
    "   - It represents the connection to a Spark cluster and is responsible for managing the execution of Spark applications.\n",
    "\n",
    "2. **Functionality:**\n",
    "   - Manages the allocation of resources in the Spark cluster, including memory and CPU.\n",
    "   - Coordinates the execution of Spark jobs, dividing the application into stages and tasks.\n",
    "   - Provides access to cluster resources and controls the creation of Resilient Distributed Datasets (RDDs).\n",
    "   - Handles configuration settings for the Spark application.\n",
    "\n",
    "3. **Creation:**\n",
    "   - Typically created once in a Spark application.\n",
    "\n",
    "   ```python\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   # Create a SparkContext\n",
    "   sc = SparkContext(\"local\", \"MySparkApplication\")\n",
    "   ```\n",
    "\n",
    "4. **Legacy:**\n",
    "   - `SparkContext` is the older and more traditional entry point in Spark, and it predates the introduction of DataFrames and Spark SQL.\n",
    "\n",
    "### SparkSession:\n",
    "\n",
    "1. **Role:**\n",
    "   - `SparkSession` is a higher-level abstraction introduced in Spark 2.0 to provide a unified entry point for reading data, working with DataFrames, and executing Spark SQL queries.\n",
    "   - It encapsulates `SparkContext` and provides additional functionalities for structured data processing.\n",
    "\n",
    "2. **Functionality:**\n",
    "   - Provides a single entry point for reading data from various structured sources, creating DataFrames, and executing Spark SQL queries.\n",
    "   - Manages the creation of DataFrames, Datasets, and TempViews for working with structured data.\n",
    "   - Encapsulates configurations, including those related to Spark SQL and Hive.\n",
    "\n",
    "3. **Creation:**\n",
    "   - Typically created once in a Spark application, similar to `SparkContext`.\n",
    "\n",
    "   ```python\n",
    "   from pyspark.sql import SparkSession\n",
    "\n",
    "   # Create a SparkSession\n",
    "   spark = SparkSession.builder.appName(\"MySparkApplication\").getOrCreate()\n",
    "   ```\n",
    "\n",
    "4. **Modern Usage:**\n",
    "   - `SparkSession` is the modern and recommended entry point for Spark applications, especially when working with structured data using DataFrames and Spark SQL.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- `SparkContext` is primarily concerned with low-level Spark functionality, resource management, and the coordination of Spark jobs.\n",
    "- `SparkSession` is a higher-level abstraction that focuses on structured data processing, providing unified access to Spark's structured APIs.\n",
    "\n",
    "In practice, many Spark applications use `SparkSession` for structured data processing while still having access to `SparkContext` for low-level operations. The `SparkSession` encapsulates a `SparkContext` internally and simplifies the overall development experience, especially for users working with structured and semi-structured data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
