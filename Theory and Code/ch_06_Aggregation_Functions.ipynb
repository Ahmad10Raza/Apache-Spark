{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aggregation Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Spark, aggregation functions are operations that perform a computation on a set of values and return a single aggregated result. These functions are commonly used in Spark DataFrames to summarize and analyze data. Here are some common aggregation functions in Spark, along with examples:**\n",
    "\n",
    "### 1. `count`:\n",
    "\n",
    "The `count` function is used to count the number of rows in a DataFrame.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", 28, \"New York\"),\n",
    "        (\"Bob\", 35, \"San Francisco\"),\n",
    "        (\"Charlie\", 22, \"Los Angeles\")]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"name\", \"age\", \"city\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Count the number of rows\n",
    "row_count = df.count()\n",
    "\n",
    "# Show the result\n",
    "print(f\"Number of rows: {row_count}\")\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "### 2. `sum`:\n",
    "\n",
    "The `sum` function is used to calculate the sum of values in a column.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Sum the values in the \"age\" column\n",
    "total_age = df.agg(sum(\"age\")).collect()[0][0]\n",
    "\n",
    "# Show the result\n",
    "print(f\"Total age: {total_age}\")\n",
    "```\n",
    "\n",
    "### 3. `avg`:\n",
    "\n",
    "The `avg` function calculates the average of values in a column.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate the average age\n",
    "average_age = df.agg(avg(\"age\")).collect()[0][0]\n",
    "\n",
    "# Show the result\n",
    "print(f\"Average age: {average_age}\")\n",
    "```\n",
    "\n",
    "### 4. `min` and `max`:\n",
    "\n",
    "The `min` and `max` functions find the minimum and maximum values in a column, respectively.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Find the minimum and maximum ages\n",
    "min_age = df.agg(min(\"age\")).collect()[0][0]\n",
    "max_age = df.agg(max(\"age\")).collect()[0][0]\n",
    "\n",
    "# Show the results\n",
    "print(f\"Minimum age: {min_age}\")\n",
    "print(f\"Maximum age: {max_age}\")\n",
    "```\n",
    "\n",
    "### 5. `groupBy` and `agg`:\n",
    "\n",
    "Combining `groupBy` with `agg` allows for more complex aggregations, such as counting occurrences of values.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by the \"city\" column and count occurrences\n",
    "city_counts = df.groupBy(\"city\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Show the result\n",
    "city_counts.show()\n",
    "```\n",
    "\n",
    "\n",
    "### 6. `groupBy` with Multiple Aggregations:\n",
    "\n",
    "You can perform multiple aggregations on different columns using `groupBy` and chaining aggregation functions.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import sum, avg, max\n",
    "\n",
    "# Group by \"city\" and calculate sum, average, and maximum age for each city\n",
    "result_grouped = df.groupBy(\"city\").agg(\n",
    "    sum(\"age\").alias(\"total_age\"),\n",
    "    avg(\"age\").alias(\"average_age\"),\n",
    "    max(\"age\").alias(\"max_age\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "result_grouped.show()\n",
    "```\n",
    "\n",
    "### 7. `groupBy` with Pivot:\n",
    "\n",
    "Pivot allows you to transform rows into columns based on a specific column's values.\n",
    "\n",
    "```python\n",
    "# Pivot the DataFrame based on the \"city\" column\n",
    "result_pivot = df.groupBy(\"name\").pivot(\"city\").agg(avg(\"age\"))\n",
    "\n",
    "# Show the result\n",
    "result_pivot.show()\n",
    "```\n",
    "\n",
    "### 8. `approxQuantile`:\n",
    "\n",
    "`approxQuantile` is used to approximate the quantiles of a numerical column.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import approxQuantile\n",
    "\n",
    "# Calculate approximate quantiles for the \"age\" column\n",
    "quantiles = df.approxQuantile(\"age\", [0.25, 0.5, 0.75], 0.1)\n",
    "\n",
    "# Show the result\n",
    "print(\"Approximate Quantiles:\", quantiles)\n",
    "```\n",
    "\n",
    "### 9. `corr`:\n",
    "\n",
    "The `corr` function calculates the Pearson correlation coefficient between two numerical columns.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "# Calculate the correlation between \"age\" and \"other_numeric_column\"\n",
    "correlation = df.agg(corr(\"age\", \"other_numeric_column\")).collect()[0][0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Correlation:\", correlation)\n",
    "```\n",
    "\n",
    "### 10. `collect_list` and `collect_set`:\n",
    "\n",
    "`collect_list` and `collect_set` are used to aggregate values into lists or sets.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "\n",
    "# Collect a list of names for each city\n",
    "result_list = df.groupBy(\"city\").agg(collect_list(\"name\").alias(\"names_list\"))\n",
    "\n",
    "# Collect a set of unique names for each city\n",
    "result_set = df.groupBy(\"city\").agg(collect_set(\"name\").alias(\"names_set\"))\n",
    "\n",
    "# Show the results\n",
    "result_list.show()\n",
    "result_set.show()\n",
    "```\n",
    "\n",
    "Certainly! Let's continue exploring more aggregation functions in Spark:\n",
    "\n",
    "### 11. `percentile`:\n",
    "\n",
    "The `percentile` function is used to calculate the specified percentiles of a numerical column.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import percentile\n",
    "\n",
    "# Calculate percentiles for the \"age\" column\n",
    "percentiles = df.agg(percentile(\"age\", [0.25, 0.5, 0.75])).collect()[0]\n",
    "\n",
    "# Show the result\n",
    "print(\"Percentiles:\", percentiles)\n",
    "```\n",
    "\n",
    "### 12. `first` and `last`:\n",
    "\n",
    "The `first` and `last` functions return the first or last value in a group, respectively.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "# Get the first and last names for each city\n",
    "result_first_last = df.groupBy(\"city\").agg(\n",
    "    first(\"name\").alias(\"first_name\"),\n",
    "    last(\"name\").alias(\"last_name\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "result_first_last.show()\n",
    "```\n",
    "\n",
    "### 13. `pivot` with Aggregation:\n",
    "\n",
    "You can use `pivot` along with aggregation functions to pivot and aggregate data simultaneously.\n",
    "\n",
    "```python\n",
    "# Pivot the DataFrame based on the \"city\" column and calculate the sum of ages for each city\n",
    "result_pivot_aggregate = df.groupBy(\"name\").pivot(\"city\").agg(sum(\"age\"))\n",
    "\n",
    "# Show the result\n",
    "result_pivot_aggregate.show()\n",
    "```\n",
    "\n",
    "### 14. `window` function:\n",
    "\n",
    "The `window` function is used for window-based aggregations, such as running totals.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Define a window specification\n",
    "window_spec = Window.orderBy(\"name\")\n",
    "\n",
    "# Calculate the running total of ages for each name\n",
    "result_running_total = df.withColumn(\"running_total\", sum(\"age\").over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "result_running_total.show()\n",
    "```\n",
    "\n",
    "### 15. Custom Aggregation:\n",
    "\n",
    "You can define custom aggregation logic using the `agg` function and a user-defined function (UDF).\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Define a custom aggregation function (e.g., sum of squared ages)\n",
    "@udf(IntegerType())\n",
    "def sum_of_squares(values):\n",
    "    return sum(x**2 for x in values)\n",
    "\n",
    "# Use the custom aggregation in the DataFrame\n",
    "result_custom_aggregation = df.groupBy(\"city\").agg(sum_of_squares(collect_list(\"age\")).alias(\"sum_of_squares\"))\n",
    "\n",
    "# Show the result\n",
    "result_custom_aggregation.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
