{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sources\n",
    "\n",
    "Apache Spark supports various data sources, allowing users to read and write data from/to different storage systems, file formats, and databases. Here are some commonly used data sources in Spark:\n",
    "\n",
    "### 1. **Hadoop Distributed File System (HDFS):**\n",
    "   - Spark can read and write data stored in HDFS, which is the distributed file system used by Hadoop.\n",
    "\n",
    "### 2. **Apache Hive:**\n",
    "   - Spark supports reading data from Hive tables, making it compatible with the Hive data warehouse.\n",
    "\n",
    "### 3. **Apache HBase:**\n",
    "   - Spark can read and write data to Apache HBase, a NoSQL database that runs on top of the Hadoop Distributed File System.\n",
    "\n",
    "### 4. **Apache Cassandra:**\n",
    "   - Spark can interact with Apache Cassandra, a distributed NoSQL database.\n",
    "\n",
    "### 5. **Amazon S3:**\n",
    "   - Spark provides connectors to read and write data from Amazon S3, a widely used cloud storage service.\n",
    "\n",
    "### 6. **Azure Blob Storage:**\n",
    "   - Similar to S3, Spark can read and write data from Azure Blob Storage, Microsoft's cloud object storage solution.\n",
    "\n",
    "### 7. **File Formats:**\n",
    "   - Spark supports various file formats, including Parquet, Avro, ORC, JSON, CSV, and more. Each format has its advantages, and users can choose the one that best suits their needs.\n",
    "\n",
    "### 8. **Apache Kafka:**\n",
    "   - Spark can consume and process data from Apache Kafka, a distributed event streaming platform.\n",
    "\n",
    "### 9. **Relational Databases:**\n",
    "   - Spark can connect to and interact with traditional relational databases such as MySQL, PostgreSQL, Oracle, and others. This is often done using JDBC connectors.\n",
    "\n",
    "### 10. **Elasticsearch:**\n",
    "    - Spark can read and write data to Elasticsearch, a distributed search and analytics engine.\n",
    "\n",
    "### 11. **JDBC Data Sources:**\n",
    "    - Spark supports reading and writing data using Java Database Connectivity (JDBC) to connect to various databases.\n",
    "\n",
    "### 12. **Graph Databases (Neo4j, etc.):**\n",
    "    - For graph-based data, Spark can interact with graph databases like Neo4j.\n",
    "\n",
    "### 13. **In-memory Data Grids (e.g., Apache Ignite):**\n",
    "    - Spark can leverage in-memory data grids for distributed in-memory processing.\n",
    "\n",
    "### 14. **Custom Data Sources:**\n",
    "    - Spark provides APIs for developers to create custom data sources, allowing integration with specialized or proprietary data storage systems.\n",
    "\n",
    "### 15. **DataFrames and Datasets:**\n",
    "    - Spark itself represents data as DataFrames and Datasets, providing a unified API for working with structured data. These can be used as sources for further processing or as sinks for storing results.\n",
    "\n",
    "Spark's flexibility in supporting various data sources makes it a versatile framework for working with diverse data ecosystems and use cases. Users can seamlessly integrate Spark with their preferred storage systems and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading & Writing On CSV File \n",
    "\n",
    "In Apache Spark, you can perform read and write operations on CSV (Comma-Separated Values) files using the `spark.read` and `DataFrame.write` API. Here's how you can do it:\n",
    "\n",
    "### Reading CSV Files:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CSVExample\").getOrCreate()\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "csv_file_path = \"path/to/your/csv/file.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- `header=True`: Specifies that the first row of the CSV file contains the header.\n",
    "- `inferSchema=True`: Infers the data types of columns automatically.\n",
    "\n",
    "### Writing to CSV Files:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CSVWriteExample\").getOrCreate()\n",
    "\n",
    "# Assume df is the DataFrame you want to write to a CSV file\n",
    "\n",
    "# Specify the path to save the CSV file\n",
    "output_csv_path = \"path/to/save/output.csv\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.write.csv(output_csv_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- `header=True`: Writes the header to the CSV file.\n",
    "- `mode=\"overwrite\"`: Specifies that if the output CSV file already exists, it should be overwritten. Other options for `mode` include `\"append\"` and `\"ignore\"`.\n",
    "\n",
    "Keep in mind that when reading and writing CSV files in Spark, you can customize the options based on your specific requirements. The `spark.read.csv` and `DataFrame.write.csv` methods provide various options for handling header, delimiter, schema, and more.\n",
    "\n",
    "Note: Make sure to replace `\"path/to/your/csv/file.csv\"` and `\"path/to/save/output.csv\"` with the actual paths in your file system.\n",
    "\n",
    "If your CSV file has a different delimiter (not a comma), you can specify it using the `sep` option. For example, if the delimiter is a semicolon, you can use `df.write.option(\"sep\", \";\").csv(output_csv_path)` when writing and `spark.read.option(\"sep\", \";\").csv(csv_file_path)` when reading. Adjust these options according to the specifics of your CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With JSON File...\n",
    "\n",
    "Working with JSON files in Apache Spark involves using the `spark.read` and `DataFrame.write` API to read and write data in JSON format. Here's a basic guide on how to perform these operations:\n",
    "\n",
    "### Reading JSON Files:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"JSONExample\").getOrCreate()\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "json_file_path = \"path/to/your/json/file.json\"\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "df = spark.read.json(json_file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "### Writing to JSON Files:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"JSONWriteExample\").getOrCreate()\n",
    "\n",
    "# Assume df is the DataFrame you want to write to a JSON file\n",
    "\n",
    "# Specify the path to save the JSON file\n",
    "output_json_path = \"path/to/save/output.json\"\n",
    "\n",
    "# Write the DataFrame to a JSON file\n",
    "df.write.json(output_json_path, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "### Options and Configurations:\n",
    "\n",
    "You can customize the behavior of reading and writing JSON files by using various options. For example:\n",
    "\n",
    "#### Reading JSON Files with Options:\n",
    "\n",
    "```python\n",
    "# Read the JSON file with options\n",
    "df = spark.read.option(\"multiLine\", \"true\").json(json_file_path)\n",
    "```\n",
    "\n",
    "Here, the `multiLine` option is set to `true` to allow parsing JSON files with multiline records.\n",
    "\n",
    "#### Writing to JSON Files with Options:\n",
    "\n",
    "```python\n",
    "# Write the DataFrame to a JSON file with options\n",
    "df.write.option(\"compression\", \"gzip\").json(output_json_path)\n",
    "```\n",
    "\n",
    "In this example, the `compression` option is set to `\"gzip\"` to compress the output JSON files.\n",
    "\n",
    "### Handling Nested JSON Structures:\n",
    "\n",
    "If your JSON file contains nested structures, Spark will automatically infer the schema. You can explore the nested structure using the `printSchema` method:\n",
    "\n",
    "```python\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "Spark will display the inferred schema, including the nested structures.\n",
    "\n",
    "Keep in mind that the specifics of reading and writing JSON files may depend on the structure of your JSON data. Customize the options based on your data format and requirements.\n",
    "\n",
    "Adjust the file paths in the examples with the actual paths in your file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 14:08:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|       Saint Martin|    2|\n",
      "|       United States|             Guinea|    2|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Romania|    3|\n",
      "|       United States|            Ireland|  268|\n",
      "|               Egypt|      United States|   13|\n",
      "|       United States|              India|   76|\n",
      "|       United States|          Singapore|   24|\n",
      "|       United States|            Grenada|   59|\n",
      "|          Costa Rica|      United States|  494|\n",
      "|             Senegal|      United States|   29|\n",
      "|              Guyana|      United States|   26|\n",
      "|       United States|   Marshall Islands|   49|\n",
      "|       United States|       Sint Maarten|  223|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   61|\n",
      "|            Anguilla|      United States|   21|\n",
      "|       United States|           Paraguay|    3|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|Turks and Caicos ...|      United States|  163|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"JSONExample\").getOrCreate()\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "json_file_path = \"/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/2011-summary.json\"\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "df = spark.read.json(json_file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 14:08:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"JSONWriteExample\").getOrCreate()\n",
    "\n",
    "# Assume df is the DataFrame you want to write to a JSON file\n",
    "\n",
    "# Specify the path to save the JSON file\n",
    "output_json_path = \"/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/2011-summary_output.json\"\n",
    "\n",
    "# Write the DataFrame to a JSON file\n",
    "df.write.json(output_json_path, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet File\n",
    "\n",
    "Parquet is a columnar storage file format that is highly optimized for use with big data processing frameworks like Apache Spark, Apache Hive, and Apache Impala. It is designed to provide better performance and storage efficiency compared to traditional row-based file formats like CSV and JSON. Parquet is particularly well-suited for analytics workloads on large datasets.\n",
    "\n",
    "Here's an example of working with Parquet files in Apache Spark:\n",
    "\n",
    "### Writing to Parquet:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ParquetExample\").getOrCreate()\n",
    "\n",
    "# Assume df is the DataFrame you want to write to a Parquet file\n",
    "\n",
    "# Specify the path to save the Parquet file\n",
    "output_parquet_path = \"path/to/save/output.parquet\"\n",
    "\n",
    "# Write the DataFrame to a Parquet file\n",
    "df.write.parquet(output_parquet_path, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "In this example, the `write.parquet` method is used to write the DataFrame to a Parquet file. The `mode=\"overwrite\"` option specifies that if the output Parquet file already exists, it should be overwritten. Other options for `mode` include `\"append\"` and `\"ignore\"`.\n",
    "\n",
    "### Reading Parquet Files:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ParquetReadExample\").getOrCreate()\n",
    "\n",
    "# Specify the path to the Parquet file\n",
    "parquet_file_path = \"path/to/your/parquet/file.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df_parquet = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "The `read.parquet` method is used to read a Parquet file into a DataFrame. Spark automatically infers the schema of the Parquet file.\n",
    "\n",
    "### Options and Configurations:\n",
    "\n",
    "You can customize the behavior of reading and writing Parquet files by using various options. For example:\n",
    "\n",
    "#### Reading Parquet Files with Options:\n",
    "\n",
    "```python\n",
    "# Read the Parquet file with options\n",
    "df_parquet = spark.read.option(\"mergeSchema\", \"true\").parquet(parquet_file_path)\n",
    "```\n",
    "\n",
    "Here, the `mergeSchema` option is set to `true` to merge multiple Parquet files with different schemas into a single DataFrame.\n",
    "\n",
    "#### Writing to Parquet Files with Options:\n",
    "\n",
    "```python\n",
    "# Write the DataFrame to a Parquet file with options\n",
    "df.write.option(\"compression\", \"snappy\").parquet(output_parquet_path)\n",
    "```\n",
    "\n",
    "In this example, the `compression` option is set to `\"snappy\"` to use the Snappy compression algorithm for the output Parquet files.\n",
    "\n",
    "Parquet files provide benefits such as better compression, schema evolution support, and predicate pushdown optimization. They are widely used in big data ecosystems for efficient data storage and processing. Adjust the file paths in the examples with the actual paths in your file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ParquetReadExample\").getOrCreate()\n",
    "\n",
    "# Specify the path to the Parquet file\n",
    "parquet_file_path = \"/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet\"\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df_parquet = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_parquet.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORC File\n",
    "\n",
    "ORC (Optimized Row Columnar) is a columnar storage file format that is highly optimized for use with big data processing frameworks like Apache Spark and Apache Hive. It is designed to provide better performance and storage efficiency compared to traditional row-based file formats like CSV and JSON. ORC is particularly well-suited for analytics workloads on large datasets.\n",
    "\n",
    "Here's an example of reading and writing ORC files in Apache Spark:\n",
    "\n",
    "### Writing to ORC:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ORCExample\").getOrCreate()\n",
    "\n",
    "# Assume df is the DataFrame you want to write to an ORC file\n",
    "\n",
    "# Specify the path to save the ORC file\n",
    "output_orc_path = \"path/to/save/output.orc\"\n",
    "\n",
    "# Write the DataFrame to an ORC file\n",
    "df.write.orc(output_orc_path, mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "In this example, the `write.orc` method is used to write the DataFrame to an ORC file. The `mode=\"overwrite\"` option specifies that if the output ORC file already exists, it should be overwritten. Other options for `mode` include `\"append\"` and `\"ignore\"`.\n",
    "\n",
    "### Reading ORC Files:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ORCReadExample\").getOrCreate()\n",
    "\n",
    "# Specify the path to the ORC file\n",
    "orc_file_path = \"path/to/your/orc/file.orc\"\n",
    "\n",
    "# Read the ORC file into a DataFrame\n",
    "df_orc = spark.read.orc(orc_file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_orc.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "The `read.orc` method is used to read an ORC file into a DataFrame. Spark automatically infers the schema of the ORC file.\n",
    "\n",
    "### Options and Configurations:\n",
    "\n",
    "You can customize the behavior of reading and writing ORC files by using various options. For example:\n",
    "\n",
    "#### Reading ORC Files with Options:\n",
    "\n",
    "```python\n",
    "# Read the ORC file with options\n",
    "df_orc = spark.read.option(\"compression\", \"zlib\").orc(orc_file_path)\n",
    "```\n",
    "\n",
    "Here, the `compression` option is set to `\"zlib\"` to use the zlib compression algorithm for the input ORC files.\n",
    "\n",
    "#### Writing to ORC Files with Options:\n",
    "\n",
    "```python\n",
    "# Write the DataFrame to an ORC file with options\n",
    "df.write.option(\"compression\", \"snappy\").orc(output_orc_path)\n",
    "```\n",
    "\n",
    "In this example, the `compression` option is set to `\"snappy\"` to use the Snappy compression algorithm for the output ORC files.\n",
    "\n",
    "ORC files provide benefits such as better compression, predicate pushdown optimization, and improved query performance. They are widely used in big data ecosystems for efficient data storage and processing. Adjust the file paths in the examples with the actual paths in your file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ORCReadExample\").getOrCreate()\n",
    "\n",
    "# Specify the path to the ORC file\n",
    "orc_file_path = \"/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/part-r-00000-2c4f7d96-e703-4de3-af1b-1441d172c80f.snappy.orc\"\n",
    "\n",
    "# Read the ORC file into a DataFrame\n",
    "df_orc = spark.read.orc(orc_file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_orc.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text File ...\n",
    "\n",
    "In Apache Spark, reading and writing text files is a common operation. Here's an example of how to read and write text files using Spark:\n",
    "\n",
    "### Writing to Text File:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TextFileWriteExample\").getOrCreate()\n",
    "\n",
    "# Assume df is the DataFrame you want to write to a text file\n",
    "\n",
    "# Specify the path to save the text file\n",
    "output_text_path = \"path/to/save/output.txt\"\n",
    "\n",
    "# Write the DataFrame to a text file\n",
    "df.write.text(output_text_path, compression=\"gzip\", mode=\"overwrite\")\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- `write.text`: The method used to write the DataFrame to a text file.\n",
    "- `output_text_path`: The path where the text file or directory will be created.\n",
    "- `compression=\"gzip\"`: Optional. Specifies the compression codec to use. Here, it's set to Gzip compression.\n",
    "- `mode=\"overwrite\"`: Specifies that if the output file or directory already exists, it should be overwritten.\n",
    "\n",
    "### Reading Text File:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TextFileReadExample\").getOrCreate()\n",
    "\n",
    "# Specify the path to the text file\n",
    "text_file_path = \"path/to/your/text/file.txt\"\n",
    "\n",
    "# Read the text file into a DataFrame\n",
    "df_text = spark.read.text(text_file_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df_text.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- `read.text`: The method used to read a text file into a DataFrame.\n",
    "- `text_file_path`: The path to the text file.\n",
    "\n",
    "### Options and Configurations:\n",
    "\n",
    "You can customize the behavior of reading and writing text files by using various options. For example:\n",
    "\n",
    "#### Reading Text File with Options:\n",
    "\n",
    "```python\n",
    "# Read the text file with options\n",
    "df_text = spark.read.option(\"header\", \"true\").text(text_file_path)\n",
    "```\n",
    "\n",
    "Here, the `header` option is set to `true` to interpret the first line of the text file as a header.\n",
    "\n",
    "#### Writing to Text File with Options:\n",
    "\n",
    "```python\n",
    "# Write the DataFrame to a text file with options\n",
    "df.write.option(\"delimiter\", \",\").text(output_text_path)\n",
    "```\n",
    "\n",
    "In this example, the `delimiter` option is set to `,` to specify a custom delimiter for the output text file.\n",
    "\n",
    "Text files are a simple and versatile format, but keep in mind that they might not be the most efficient for large-scale data processing compared to columnar formats like Parquet or ORC. Adjust the file paths and options in the examples based on your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
