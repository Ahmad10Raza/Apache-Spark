{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data in PySpark involves using the `SparkSession` object to create a DataFrame, which is a distributed collection of data organized into named columns. PySpark supports reading data from various sources, such as CSV, JSON, Parquet, Avro, and more. Let's go through the steps and terms involved in reading data using PySpark:\n",
    "\n",
    "### 1. **Initialize a Spark Session:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "```\n",
    "\n",
    "Here, we create a `SparkSession` named \"example,\" which is the entry point to programming Spark with the DataFrame and SQL API.\n",
    "\n",
    "### 2. **Read Data into a DataFrame:**\n",
    "\n",
    "You can use the `read` attribute of the `SparkSession` to read data from different formats. For example, reading from a CSV file:\n",
    "\n",
    "```python\n",
    "# Read CSV data into a DataFrame\n",
    "df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "- **`csv(\"path/to/your/file.csv\")`:**\n",
    "  - Specifies the path to the CSV file.\n",
    "- **`header=True`:**\n",
    "  - Specifies that the first row of the CSV file contains the column names.\n",
    "- **`inferSchema=True`:**\n",
    "  - Infers the data types of the columns.\n",
    "\n",
    "### 3. **Show the DataFrame:**\n",
    "\n",
    "```python\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()\n",
    "```\n",
    "\n",
    "The `show()` function displays the first 20 rows of the DataFrame in a tabular format.\n",
    "\n",
    "### 4. **DataFrame Schema:**\n",
    "\n",
    "```python\n",
    "# Display the schema of the DataFrame\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "The `printSchema()` function prints the schema of the DataFrame, showing the data types of each column.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read CSV data into a DataFrame\n",
    "df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Display the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "Replace \"path/to/your/file.csv\" with the actual path to your CSV file.\n",
    "\n",
    "### Terms:\n",
    "\n",
    "- **Spark Session (`SparkSession`):**\n",
    "  - The entry point to programming Spark. It allows the creation of DataFrames and executing SQL queries.\n",
    "\n",
    "- **DataFrame:**\n",
    "  - A distributed collection of data organized into named columns. It is the primary abstraction in PySpark.\n",
    "\n",
    "- **Read API (`read`):**\n",
    "  - An attribute of `SparkSession` used for reading data from external sources.\n",
    "\n",
    "- **CSV (Comma-Separated Values):**\n",
    "  - A popular plain-text format for tabular data, where each line of the file corresponds to a row, and values are separated by commas.\n",
    "\n",
    "- **Header:**\n",
    "  - The first row of a CSV file that contains column names.\n",
    "\n",
    "- **InferSchema:**\n",
    "  - An option to automatically infer the data types of columns when reading data.\n",
    "\n",
    "- **Schema:**\n",
    "  - The structure that defines the data types of each column in a DataFrame.\n",
    "\n",
    "- **`show()`:**\n",
    "  - A function used to display the first few rows of a DataFrame.\n",
    "\n",
    "- **`printSchema()`:**\n",
    "  - A function used to display the schema of a DataFrame.\n",
    "\n",
    "These terms and steps provide a basic overview of how to read data into a PySpark DataFrame. Depending on the format of your data, you may use different read functions (e.g., `read.json()`, `read.parquet()`, etc.) with specific options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/10 00:48:43 WARN Utils: Your hostname, blackheart resolves to a loopback address: 127.0.1.1; using 192.168.74.222 instead (on interface wlp1s0)\n",
      "23/11/10 00:48:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/10 00:48:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read CSV data into a DataFrame\n",
    "df = spark.read.csv(\"/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/2015-summary.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the schema of the DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    "  .read\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data In `SQL` Way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read CSV data into a DataFrame\n",
    "df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Use SQL queries to interact with the data\n",
    "result = spark.sql(\"SELECT * FROM my_table WHERE age > 25\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameWay = flightData2015\\\n",
    "  .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#57], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#57, 200), ENSURE_REQUIREMENTS, [plan_id=110]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#57], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#57] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#57], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#57, 200), ENSURE_REQUIREMENTS, [plan_id=123]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#57], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#57] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()\n",
    "dataFrameWay.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|   DEST_COUNTRY_NAME|count(1)|\n",
      "+--------------------+--------+\n",
      "|            Anguilla|       1|\n",
      "|              Russia|       1|\n",
      "|            Paraguay|       1|\n",
      "|             Senegal|       1|\n",
      "|              Sweden|       1|\n",
      "|            Kiribati|       1|\n",
      "|              Guyana|       1|\n",
      "|         Philippines|       1|\n",
      "|            Djibouti|       1|\n",
      "|            Malaysia|       1|\n",
      "|           Singapore|       1|\n",
      "|                Fiji|       1|\n",
      "|              Turkey|       1|\n",
      "|                Iraq|       1|\n",
      "|             Germany|       1|\n",
      "|              Jordan|       1|\n",
      "|               Palau|       1|\n",
      "|Turks and Caicos ...|       1|\n",
      "|              France|       1|\n",
      "|              Greece|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
