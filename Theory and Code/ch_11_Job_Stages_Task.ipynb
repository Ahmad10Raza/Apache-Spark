{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Job in Spark, stage and Task?\n",
    "\n",
    "In Apache Spark, a \"job\" refers to the entire computation triggered by an action in a Spark application. It represents a set of transformations on data that are executed in a distributed and parallel manner across the nodes of a Spark cluster. A Spark job consists of one or more stages, where each stage represents a set of transformations that can be executed in parallel.\n",
    "\n",
    "Here are key concepts related to Spark jobs:\n",
    "\n",
    "1. **Action Trigger:**\n",
    "   - Spark jobs are triggered by actions. Actions are operations on RDDs or DataFrames that trigger the execution of the entire computation plan built by transformations.\n",
    "\n",
    "   ```python\n",
    "   # Example of an action triggering a job\n",
    "   result = rdd.reduce(lambda x, y: x + y)\n",
    "   ```\n",
    "\n",
    "   In this example, the `reduce` action triggers the execution of the transformations defined on the `rdd`.\n",
    "\n",
    "2. **Stages:**\n",
    "   - A job is divided into one or more stages. Each stage consists of a set of transformations that can be executed in parallel. Stages are determined based on the presence of wide transformations that require shuffling of data between partitions.\n",
    "\n",
    "3. **Tasks:**\n",
    "   - A stage is further divided into tasks, where each task represents the smallest unit of parallel execution. Tasks are assigned to individual partitions of data, and they execute on the worker nodes of the Spark cluster.\n",
    "\n",
    "4. **Shuffle:**\n",
    "   - A Spark job may involve shuffling of data, which is the process of redistributing data across the partitions. This occurs when wide transformations like `groupByKey` or `reduceByKey` are used, requiring data to be exchanged between partitions.\n",
    "\n",
    "5. **Job Execution Plan:**\n",
    "   - Spark builds a directed acyclic graph (DAG) representing the execution plan for the entire job. The DAG includes transformations and dependencies between them. The execution plan is optimized to minimize data movement and improve performance.\n",
    "\n",
    "6. **Fault Tolerance:**\n",
    "   - Spark provides fault tolerance for jobs through lineage information. Each RDD or DataFrame keeps track of its lineage, which is the sequence of transformations that led to its creation. If a partition is lost due to a node failure, Spark can recompute the lost partition using the lineage information.\n",
    "\n",
    "7. **Job Monitoring:**\n",
    "   - Spark provides monitoring tools, such as the Spark UI and Spark History Server, to monitor the progress and performance of jobs. These tools display information about completed and ongoing jobs, stages, and tasks.\n",
    "\n",
    "![Job](/home/blackheart/Documents/Data/Apache-Spark/Images/Job.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the concepts of jobs, stages, and tasks in Apache Spark with an example:\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a simple Spark application that performs a few transformations and an action on an RDD.\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"SparkExample\")\n",
    "\n",
    "# Create an RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Transformation 1: Map to double each element\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Transformation 2: Filter to keep only even numbers\n",
    "filtered_rdd = mapped_rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Action: Compute the sum\n",
    "result = filtered_rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n",
    "```\n",
    "\n",
    "### Concepts:\n",
    "\n",
    "1. **Job:**\n",
    "   - In this example, the entire computation triggered by the `reduce` action is a Spark job.\n",
    "   - The job includes all the transformations (in this case, `map` and `filter`) and the final action (`reduce`).\n",
    "\n",
    "2. **Stages:**\n",
    "   - The job is divided into stages based on the presence of wide transformations that require shuffling of data between partitions.\n",
    "   - In this example, there are two stages:\n",
    "     - **Stage 1:** The `map` transformation, which is narrow and can be executed in parallel.\n",
    "     - **Stage 2:** The `filter` transformation and the final `reduce` action, which may involve shuffling and require a separate stage.\n",
    "\n",
    "3. **Tasks:**\n",
    "   - Each stage is further divided into tasks, representing the smallest unit of parallel execution.\n",
    "   - Tasks are assigned to partitions of data and executed on worker nodes.\n",
    "   - In Stage 1, there is one task for each partition (assuming the default parallelism).\n",
    "   - In Stage 2, the number of tasks depends on the number of partitions after the `filter` transformation.\n",
    "\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Job:**\n",
    "   - The entire execution triggered by the `reduce` action is a single Spark job.\n",
    "\n",
    "2. **Stages:**\n",
    "   - Stage 1: `map` transformation is a narrow transformation, so it forms a single stage.\n",
    "   - Stage 2: `filter` transformation and the final `reduce` action may involve shuffling, so they form a separate stage.\n",
    "\n",
    "3. **Tasks:**\n",
    "   - Tasks within Stage 1 execute `map` in parallel for each partition.\n",
    "   - Tasks within Stage 2 execute `filter` and the final `reduce` in parallel for each partition.\n",
    "\n",
    "This example illustrates how a Spark job is divided into stages, and each stage consists of tasks that can be executed in parallel. The concept of stages allows Spark to optimize the execution plan and efficiently distribute computation across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Job Count?\n",
    "Calculating the number of jobs created by a Spark program involves understanding the program's structure, specifically the actions and transformations used. Each action typically triggers the execution of a job, and transformations within the program may cause the creation of multiple stages within a job.\n",
    "\n",
    "Here are some guidelines to help you identify the number of jobs in a Spark program:\n",
    "\n",
    "1. **Actions Trigger Jobs:**\n",
    "   - Look for actions in your Spark program. Actions, such as `count`, `collect`, `saveAsTextFile`, or any operation that requires materializing the data, generally trigger the execution of a job.\n",
    "\n",
    "2. **Stages Within a Job:**\n",
    "   - Examine the transformations in your program. If there are wide transformations (e.g., `groupByKey`, `reduceByKey`) or actions that involve shuffling, these may introduce multiple stages within a job.\n",
    "\n",
    "3. **Spark UI:**\n",
    "   - Use the Spark UI to monitor job execution. When you run your Spark application, the Spark UI provides detailed information about completed jobs, stages, and tasks. You can access the Spark UI at `http://<driver-node>:4040` by default.\n",
    "\n",
    "4. **Program Structure:**\n",
    "   - Programs with multiple actions or multiple distinct computations may create more than one job.\n",
    "\n",
    "Here's a simple example:\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"JobCountExample\")\n",
    "\n",
    "# Create an RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Transformation 1: Map to double each element\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Action 1: Count the number of elements\n",
    "count1 = mapped_rdd.count()\n",
    "\n",
    "# Transformation 2: Filter to keep only even numbers\n",
    "filtered_rdd = mapped_rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Action 2: Collect the results\n",
    "result = filtered_rdd.collect()\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- There are two actions (`count1` and `result`), so at least two jobs will be triggered.\n",
    "- The first action may involve multiple stages, as it depends on the transformations preceding it.\n",
    "- The second action might be part of the same job or a separate one, depending on the structure of the execution plan.\n",
    "\n",
    "By examining the structure of your Spark program and understanding the impact of actions and transformations, you can estimate the number of jobs created during its execution. Monitoring the Spark UI during the execution of your program provides detailed insights into job and stage information.\n",
    "\n",
    "## List of Action\n",
    "\n",
    "Actions in Spark are operations that trigger the execution of the computation plan built by transformations. Each action typically results in the creation of one or more jobs. Here is a list of common actions in Spark:\n",
    "\n",
    "1. **`collect()`**\n",
    "   - Returns all the elements of the RDD or DataFrame as an array to the driver program.\n",
    "\n",
    "2. **`count()`**\n",
    "   - Returns the number of elements in the RDD or DataFrame.\n",
    "\n",
    "3. **`first()`**\n",
    "   - Returns the first element of the RDD or DataFrame.\n",
    "\n",
    "4. **`take(n)`**\n",
    "   - Returns the first `n` elements of the RDD or DataFrame.\n",
    "\n",
    "5. **`reduce(func)`**\n",
    "   - Aggregates the elements of the RDD or DataFrame using a specified reduce function.\n",
    "\n",
    "6. **`foreach(func)`**\n",
    "   - Applies a function to each element of the RDD or DataFrame. Commonly used for side-effect operations.\n",
    "\n",
    "7. **`saveAsTextFile(path)`**\n",
    "   - Writes the elements of the RDD or DataFrame as text files in the specified path.\n",
    "\n",
    "8. **`saveAsSequenceFile(path)`**\n",
    "   - Writes the elements of the RDD as Hadoop SequenceFiles in the specified path.\n",
    "\n",
    "9. **`saveAsObjectFile(path)`**\n",
    "   - Writes the elements of the RDD as serialized Java objects in the specified path.\n",
    "\n",
    "10. **`countByKey()`**\n",
    "    - Only applicable to RDDs of key-value pairs. Returns a map of each unique key and its count.\n",
    "\n",
    "11. **`collectAsMap()`**\n",
    "    - Only applicable to RDDs of key-value pairs. Returns the elements of the RDD as a map.\n",
    "\n",
    "12. **`lookup(key)`**\n",
    "    - Only applicable to RDDs of key-value pairs. Returns all values associated with the specified key.\n",
    "\n",
    "13. **`takeOrdered(n, key=None)`**\n",
    "    - Returns the first `n` elements of the RDD or DataFrame based on their natural order or a custom key.\n",
    "\n",
    "14. **`top(n, key=None)`**\n",
    "    - Returns the top `n` elements of the RDD or DataFrame based on their natural order or a custom key.\n",
    "\n",
    "15. **`countByValue()`**\n",
    "    - Returns the count of each unique element in the RDD as a map.\n",
    "\n",
    "16. **`max()`**\n",
    "    - Returns the maximum element of the RDD or DataFrame.\n",
    "\n",
    "17. **`min()`**\n",
    "    - Returns the minimum element of the RDD or DataFrame.\n",
    "\n",
    "18. **`sum()`**\n",
    "    - Returns the sum of the elements in the RDD or DataFrame.\n",
    "\n",
    "19. **`mean()`**\n",
    "    - Returns the mean (average) of the elements in the RDD or DataFrame.\n",
    "\n",
    "20. **`stats()`**\n",
    "    - Returns a `StatCounter` object that provides statistics (mean, variance, etc.) about the elements in the RDD or DataFrame.\n",
    "\n",
    "These actions are used to trigger the execution of Spark computations and obtain results. Keep in mind that some actions, especially those involving large datasets, can be resource-intensive, so it's essential to use them judiciously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating `Stage` and `Task`?\n",
    "\n",
    "Calculating the number of stages and tasks in Spark involves understanding the transformations in your Spark program and how they impact the execution plan. Here are guidelines to help you estimate the number of stages and tasks:\n",
    "\n",
    "### Stages:\n",
    "\n",
    "1. **Wide Transformations:**\n",
    "   - Look for wide transformations like `groupByKey`, `reduceByKey`, or any operation that requires shuffling of data. These transformations typically introduce a new stage in the execution plan.\n",
    "\n",
    "2. **Actions:**\n",
    "   - Actions generally delineate stages. Each action triggers the execution of the entire lineage leading up to that action, and each transformation that requires shuffling may introduce a new stage.\n",
    "\n",
    "3. **Spark UI:**\n",
    "   - Utilize the Spark UI (`http://<driver-node>:4040` by default) during the execution of your Spark program. The UI provides a detailed breakdown of stages, their status, and the number of tasks.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Partitions:**\n",
    "   - The number of tasks within a stage is often equal to the number of partitions of the RDD or DataFrame involved in that stage.\n",
    "\n",
    "2. **Parallelism:**\n",
    "   - Parallelism is influenced by the number of cores available on your cluster. Spark tries to parallelize the computation by assigning one task per partition to each available core.\n",
    "\n",
    "3. **Configuration:**\n",
    "   - The `spark.default.parallelism` configuration parameter in Spark influences the default number of partitions created for RDDs in operations that transform them (e.g., `map`, `filter`). You can set this parameter to control the number of tasks created during certain transformations.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider the following Spark program:\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"StagesTasksExample\")\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "filtered_rdd = mapped_rdd.filter(lambda x: x % 2 == 0)\n",
    "result = filtered_rdd.collect()\n",
    "\n",
    "sc.stop()\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- There is one action (`collect`), which triggers the execution of the entire computation plan.\n",
    "- There are two transformations (`map` and `filter`).\n",
    "- Depending on the default parallelism or configuration settings, there may be two stages (one for each transformation).\n",
    "- The number of tasks in each stage is determined by the number of partitions in the RDD involved in that stage.\n",
    "\n",
    "To get a detailed breakdown, use the Spark UI during the execution of your program. It provides information on stages, tasks, and their progress."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
