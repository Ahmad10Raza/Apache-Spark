{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL?\n",
    "\n",
    "Spark SQL is a module in Apache Spark that provides a programming interface for working with structured and semi-structured data using SQL (Structured Query Language). It enables users to seamlessly integrate SQL queries with their Spark programs, allowing them to perform complex data analysis tasks using the power of Spark's distributed computing capabilities.\n",
    "\n",
    "Key features and components of Spark SQL include:\n",
    "\n",
    "1. **DataFrame API:**\n",
    "   - Spark SQL introduces the concept of DataFrames, which are distributed collections of data organized into named columns. DataFrames provide a higher-level abstraction compared to RDDs (Resilient Distributed Datasets) and are designed to work seamlessly with Spark SQL.\n",
    "\n",
    "2. **SQL Queries:**\n",
    "   - Spark SQL allows users to execute SQL queries on DataFrames. Users can express complex transformations and aggregations using SQL syntax, providing a familiar interface for those who are already familiar with SQL.\n",
    "\n",
    "3. **Hive Compatibility:**\n",
    "   - Spark SQL is compatible with Apache Hive, allowing users to run Hive queries and access Hive UDFs (User-Defined Functions) within Spark applications. This compatibility makes it easier for organizations that have existing Hive queries to transition to Spark.\n",
    "\n",
    "4. **Catalyst Optimizer:**\n",
    "   - Spark SQL includes the Catalyst optimizer, a powerful engine that optimizes the execution plan of Spark SQL queries. Catalyst optimizes the logical and physical execution plans of Spark SQL queries to improve performance.\n",
    "\n",
    "5. **DataSource API:**\n",
    "   - Spark SQL provides a DataSource API that allows users to read and write data in various formats and storage systems. This includes support for reading and writing data in Parquet, Avro, ORC, JSON, CSV, and more.\n",
    "\n",
    "6. **Unified Data Access:**\n",
    "   - With Spark SQL, users can seamlessly switch between DataFrame and SQL API, providing a unified programming interface. This flexibility allows users to choose the API that best suits their requirements for a particular task.\n",
    "\n",
    "7. **Structured Streaming:**\n",
    "   - Spark SQL extends its capabilities to structured streaming, enabling users to process streaming data using SQL queries on DataFrames.\n",
    "\n",
    "Here's a simple example of using Spark SQL:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkSQLExample\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame from a CSV file\n",
    "df = spark.read.csv(\"path/to/your/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Perform a SQL query on the DataFrame\n",
    "result = spark.sql(\"SELECT * FROM my_table WHERE age > 25\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "In this example, a DataFrame is created from a CSV file, registered as a temporary table, and then a SQL query is executed on that table using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE flights (\\nDEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\\nUSING JSON OPTIONS (path '/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/2011-summary.json')\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''CREATE TABLE flights (\n",
    "DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "USING JSON OPTIONS (path '/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/2011-summary.json')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/11 16:31:37 WARN Utils: Your hostname, blackheart resolves to a loopback address: 127.0.1.1; using 192.168.144.222 instead (on interface wlp1s0)\n",
      "23/11/11 16:31:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/11 16:31:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 22|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 22|\n",
      "+-------+---+\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Bob| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 25|    1|\n",
      "| 30|    1|\n",
      "| 22|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Charlie| 22|\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "+-------+---+\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| Bob| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| name|age|profession|\n",
      "+-----+---+----------+\n",
      "|Alice| 25|  Engineer|\n",
      "|  Bob| 30|    Doctor|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 22|\n",
      "|  David| 28|\n",
      "|    Eva| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkSQLExample\").getOrCreate()\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Register the DataFrame as a temporary table\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# 1. Select Operation\n",
    "result_select = df.select(\"name\", \"age\")\n",
    "result_select.show()\n",
    "\n",
    "# 2. Filter Operation\n",
    "result_filter = df.filter(df[\"age\"] > 25)\n",
    "result_filter.show()\n",
    "\n",
    "# 3. GroupBy and Aggregation\n",
    "result_groupby = df.groupBy(\"age\").count()\n",
    "result_groupby.show()\n",
    "\n",
    "# 4. Sorting\n",
    "result_sort = df.orderBy(\"age\")\n",
    "result_sort.show()\n",
    "\n",
    "# 5. SQL Queries\n",
    "result_sql = spark.sql(\"SELECT name, age FROM people WHERE age > 25\")\n",
    "result_sql.show()\n",
    "\n",
    "# 6. Join Operation\n",
    "df2 = spark.createDataFrame([(\"Alice\", \"Engineer\"), (\"Bob\", \"Doctor\")], [\"name\", \"profession\"])\n",
    "result_join = df.join(df2, \"name\")\n",
    "result_join.show()\n",
    "\n",
    "# 7. Union Operation\n",
    "df3 = spark.createDataFrame([(\"David\", 28), (\"Eva\", 35)], [\"name\", \"age\"])\n",
    "result_union = df.union(df3)\n",
    "result_union.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In Spark SQL, you can perform various operations beyond the basic querying and manipulation of DataFrames. Here are examples of some additional operations:\n",
    "\n",
    "### 1. Creating External Tables:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ExternalTableExample\").getOrCreate()\n",
    "\n",
    "# Assuming 'external_data' is an external table in Hive or another supported metastore\n",
    "spark.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS external_data (name STRING, age INT) LOCATION '/path/to/external/table'\")\n",
    "```\n",
    "\n",
    "This example creates an external table named `external_data` pointing to data located at `/path/to/external/table`. The actual table creation syntax may vary based on the underlying metastore (Hive, etc.) and the data source.\n",
    "\n",
    "### 2. Inserting into Tables:\n",
    "\n",
    "```python\n",
    "# Assuming 'people' is a registered DataFrame or table\n",
    "df_insert = spark.createDataFrame([(\"John\", 28), (\"Emma\", 32)], [\"name\", \"age\"])\n",
    "\n",
    "# Insert data into an existing table\n",
    "df_insert.write.insertInto(\"people\")\n",
    "```\n",
    "\n",
    "This example inserts data from a DataFrame (`df_insert`) into an existing table named `people`.\n",
    "\n",
    "### 3. Describing Table Metadata:\n",
    "\n",
    "```python\n",
    "# Describe the metadata of a table\n",
    "spark.sql(\"DESCRIBE EXTENDED people\").show(truncate=False)\n",
    "```\n",
    "\n",
    "This SQL query provides detailed information about the structure and metadata of the `people` table.\n",
    "\n",
    "### 4. Refreshing Table Metadata:\n",
    "\n",
    "```python\n",
    "# Refresh the metadata of a table\n",
    "spark.sql(\"REFRESH TABLE people\")\n",
    "```\n",
    "\n",
    "This operation is useful when the underlying data of a table has changed, and you want to update the metadata.\n",
    "\n",
    "### 5. Dropping Tables:\n",
    "\n",
    "```python\n",
    "# Drop a table\n",
    "spark.sql(\"DROP TABLE IF EXISTS people\")\n",
    "```\n",
    "\n",
    "This SQL query drops the `people` table if it exists.\n",
    "\n",
    "### 6. Caching Tables:\n",
    "\n",
    "```python\n",
    "# Cache a table in memory\n",
    "spark.sql(\"CACHE TABLE people\")\n",
    "```\n",
    "\n",
    "This operation caches the `people` table in memory, improving the performance of subsequent queries.\n",
    "\n",
    "### 7. Uncaching Tables:\n",
    "\n",
    "```python\n",
    "# Uncache a table from memory\n",
    "spark.sql(\"UNCACHE TABLE people\")\n",
    "```\n",
    "\n",
    "This operation removes the table from the in-memory cache.\n",
    "\n",
    "Remember to adapt these examples based on your specific use case, table structures, and storage systems. The syntax might vary depending on the underlying storage system (Hive, HDFS, etc.) and the Spark version you are using."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
