{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics Structure Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitionally, a DataFrame consists of a series of records (like rows in a table), that are of type\n",
    "Row, and a number of columns (like columns in a spreadsheet) that represent a computation\n",
    "expression that can be performed on each individual record in the Dataset. Schemas define the\n",
    "name as well as the type of data in each column. Partitioning of the DataFrame defines the\n",
    "layout of the DataFrame or Dataset’s physical distribution across the cluster. The partitioning\n",
    "scheme defines how that is allocated. You can set this to be based on values in a certain column\n",
    "or nondeterministically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV data into a DataFrame\n",
    "df = spark.read.csv(\"/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/2015-summary.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas\n",
    "**A schema defines the column names and types of a DataFrame. We can either let a data source\n",
    "define the schema (called schema-on-read) or we can define it explicitly ourselves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Own Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    ".load(\"/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark, you can create your own schemas when working with DataFrames. A schema defines the structure of your data, specifying the names and data types of columns. Here's how you can create your own schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,LongType\n",
    "\n",
    "\n",
    "\n",
    "# Define your own schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),    # True means the column can have null values\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Read data with the custom schema\n",
    "# df = spark.read.csv(\"path/to/your/file.csv\", header=True, schema=custom_schema)\n",
    "df = spark.read.csv(\"/home/blackheart/Documents/Data/Apache-Spark/Data/flight_data/2015-summary.csv\", header=True, schema=custom_schema)\n",
    "\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns and Expression\n",
    "\n",
    "In PySpark, columns and expressions play a crucial role in manipulating and transforming data within DataFrames. A column represents a single column of data in a DataFrame, and expressions are operations or transformations applied to columns. Here's an explanation with examples:\n",
    "\n",
    "### Columns:\n",
    "\n",
    "- **Creating Columns:**\n",
    "  - Columns can be created in several ways, including by referencing existing columns or applying operations on them.\n",
    "\n",
    "  ```python\n",
    "  from pyspark.sql import SparkSession\n",
    "  from pyspark.sql.functions import col\n",
    "\n",
    "  # Create a Spark session\n",
    "  spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "  # Read data into a DataFrame\n",
    "  df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "  # Reference an existing column\n",
    "  name_col = df[\"name\"]\n",
    "\n",
    "  # Create a new column using an operation\n",
    "  age_squared_col = df[\"age\"] ** 2\n",
    "\n",
    "  # Using the `col` function\n",
    "  city_col = col(\"city\")\n",
    "  ```\n",
    "\n",
    "### Expressions:\n",
    "\n",
    "- **Defining Expressions:**\n",
    "  - Expressions represent transformations or operations on columns. PySpark provides a rich set of built-in functions that can be used in expressions.\n",
    "\n",
    "  ```python\n",
    "  from pyspark.sql.functions import expr\n",
    "\n",
    "  # Using expressions\n",
    "  expr_expr = expr(\"age * 2\")\n",
    "  ```\n",
    "\n",
    "### Example Workflow:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read data into a DataFrame\n",
    "df = spark.read.csv(\"path/to/your/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Columns\n",
    "name_col = df[\"name\"]\n",
    "age_squared_col = df[\"age\"] ** 2\n",
    "city_col = col(\"city\")\n",
    "\n",
    "# Expressions\n",
    "expr_expr = expr(\"age * 2\")\n",
    "\n",
    "# Apply transformations using columns and expressions\n",
    "result_df = df.select(name_col, age_squared_col.alias(\"age_squared\"), city_col, expr_expr.alias(\"double_age\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "- Columns (`name_col`, `age_squared_col`, `city_col`) are created by referencing existing columns or applying operations.\n",
    "- Expressions (`expr_expr`) are created using the `expr` function.\n",
    "- Transformations are applied to the DataFrame using `select` and aliases are used to rename the resulting columns.\n",
    "- The `show` action is used to display the transformed DataFrame.\n",
    "\n",
    "Understanding columns and expressions is crucial for performing a wide range of data transformations and manipulations in PySpark. They allow you to create complex operations that can be efficiently executed on large-scale distributed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Rows\n",
    "You can create rows by manually instantiating a Row object with the values that belong in each\n",
    "column. It’s important to note that only DataFrames have schemas. Rows themselves do not have\n",
    "schemas. This means that if you create a Row manually, you must specify the values in the same\n",
    "order as the schema of the DataFrame to which they might be appended (we will see this when\n",
    "we discuss creating DataFrames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[0]\n",
    "myRow[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|NULL|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"some\", StringType(), True),\n",
    "StructField(\"col\", StringType(), True),\n",
    "StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|  Alice| 28|     New York|\n",
      "|    Bob| 35|San Francisco|\n",
      "|Charlie| 22|  Los Angeles|\n",
      "+-------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", 28, \"New York\"),\n",
    "        (\"Bob\", 35, \"San Francisco\"),\n",
    "        (\"Charlie\", 22, \"Los Angeles\")]\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select and selectExpr\n",
    "select and selectExpr allow you to do the DataFrame equivalent of SQL queries on a table of\n",
    "data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 28|\n",
      "|    Bob| 35|\n",
      "|Charlie| 22|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use select to choose specific columns\n",
    "selected_df = df.select(\"name\", \"age\")\n",
    "selected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+-------------+\n",
      "|   name|age|double_age|   upper_city|\n",
      "+-------+---+----------+-------------+\n",
      "|  Alice| 28|        56|     NEW YORK|\n",
      "|    Bob| 35|        70|SAN FRANCISCO|\n",
      "|Charlie| 22|        44|  LOS ANGELES|\n",
      "+-------+---+----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use selectExpr for more complex transformations\n",
    "transformed_df = df.selectExpr(\"name\", \"age\", \"age * 2 as double_age\", \"UPPER(city) as upper_city\")\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "* We use the select function to choose specific columns (\"name\" and \"age\") from the original DataFrame.\n",
    "* We use the selectExpr function for more complex transformations, such as creating a new column \"double_age\" and applying the UPPER function to the \"city\" column.\n",
    "* The show method is used to display the selected and transformed DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to Spark Types\n",
    "\n",
    "When you create a DataFrame in PySpark, the data is automatically converted to Spark types. However, if you want to explicitly convert columns to specific Spark types, you can use the `withColumn` method along with casting functions from `pyspark.sql.functions`. Here's an example:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", \"28\", \"New York\"),\n",
    "        (\"Bob\", \"35\", \"San Francisco\"),\n",
    "        (\"Charlie\", \"22\", \"Los Angeles\")]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"name\", \"age\", \"city\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Convert \"age\" column to IntegerType\n",
    "df = df.withColumn(\"age\", df[\"age\"].cast(IntegerType()))\n",
    "\n",
    "# Convert \"name\" and \"city\" columns to StringType\n",
    "df = df.withColumn(\"name\", df[\"name\"].cast(StringType()))\n",
    "df = df.withColumn(\"city\", df[\"city\"].cast(StringType()))\n",
    "\n",
    "# Show the DataFrame with converted types\n",
    "print(\"DataFrame with Converted Types:\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "- We use the `cast` method to explicitly convert the \"age\" column to `IntegerType`.\n",
    "- We also use `cast` to convert the \"name\" and \"city\" columns to `StringType`.\n",
    "- The `printSchema` method is used to display the schema of the DataFrame after the type conversions.\n",
    "- The `show` method is used to display the content of the DataFrame after the type conversions.\n",
    "\n",
    "Keep in mind that Spark will automatically infer the data types when you create a DataFrame. Explicit type conversions are necessary when you want to ensure that the columns have the correct Spark types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+---------+\n",
      "| name|age|         city|numberOne|\n",
      "+-----+---+-------------+---------+\n",
      "|Alice| 28|     New York|        1|\n",
      "|  Bob| 35|San Francisco|        1|\n",
      "+-----+---+-------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "# df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'city']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"numberOne\", \"Loser\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|  Alice| 28|     New York|\n",
      "|    Bob| 35|San Francisco|\n",
      "|Charlie| 22|  Los Angeles|\n",
      "+-------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reserved Characters and Keywords\n",
    "One thing that you might come across is reserved characters like spaces or dashes in column\n",
    "names. Handling these means escaping column names appropriately. In Spark, we do this by\n",
    "using backtick (`) characters. Let’s use withColumn, which you just learned about to create a\n",
    "column with reserved characters. We’ll show two examples—in the one shown here, we don’t\n",
    "need escape characters, but in the next one, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "dfWithLongColName = df.withColumn(\n",
    "\"name\",\n",
    "expr(\"city\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Sensitivity\n",
    "By default Spark is case insensitive; however, you can make Spark case sensitive by setting the\n",
    "configuration:\n",
    "-- in SQL\n",
    "```set spark.sql.caseSensitive true```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Columns\n",
    "Now that we’ve created this column, let’s take a look at how we can remove columns from\n",
    "DataFrames. You likely already noticed that we can do this by using select. However, there is\n",
    "also a dedicated method called drop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns\n",
    "# We can drop multiple columns by passing in multiple columns as arguments:\n",
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing a Column’s Type (cast)\n",
    "Sometimes, we might need to convert from one type to another; for example, if we have a set of\n",
    "StringType that should be integers. We can convert columns from one type to another by\n",
    "casting the column from one type to another. For instance, let’s convert our count column from\n",
    "an integer to a type Long:\n",
    "\n",
    "```df.withColumn(\"count2\", col(\"count\").cast(\"long\"))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Rows\n",
    "To filter rows, we create an expression that evaluates to true or false. You then filter out the rows\n",
    "with an expression that is equal to false. The most common way to do this with DataFrames is to\n",
    "create either an expression as a String or build an expression by using a set of column\n",
    "manipulations. There are two methods to perform this operation: you can use where or filter\n",
    "and they both will perform the same operation and accept the same argument types when used\n",
    "with DataFrames. We will stick to where because of its familiarity to SQL; however, filter is\n",
    "valid as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+\n",
      "| name|age|         city|\n",
      "+-----+---+-------------+\n",
      "|Alice| 28|     New York|\n",
      "|  Bob| 35|San Francisco|\n",
      "+-----+---+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+---+----+\n",
      "|name|age|city|\n",
      "+----+---+----+\n",
      "+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"age\") > 2).show(2)\n",
    "df.where(\"age < 2\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Unique Rows\n",
    "A very common use case is to extract the unique or distinct values in a DataFrame. These values\n",
    "can be in one or more columns. The way we do this is by using the distinct method on a\n",
    "DataFrame, which allows us to deduplicate any rows that are in that DataFrame. For instance,\n",
    "let’s get the unique origins in our dataset. This, of course, is a transformation that will return a\n",
    "new DataFrame with only unique rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"name\", \"age\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Samples\n",
    "Sometimes, you might just want to sample some random records from your DataFrame. You can\n",
    "do this by using the sample method on a DataFrame, which makes it possible for you to specify\n",
    "a fraction of rows to extract from a DataFrame and whether you’d like to sample with or without\n",
    "replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Splits\n",
    "Random splits can be helpful when you need to break up your DataFrame into a random “splits”\n",
    "of the original DataFrame. This is often used with machine learning algorithms to create training,\n",
    "validation, and test sets. In this next example, we’ll split our DataFrame into two different\n",
    "DataFrames by setting the weights by which we will split the DataFrame (these are the\n",
    "arguments to the function). Because this method is designed to be randomized, we will also\n",
    "specify a seed (just replace seed with a number of your choosing in the code block). It’s\n",
    "important to note that if you don’t specify a proportion for each DataFrame that adds up to one,\n",
    "they will be normalized so that they do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "dataFrames[0].count() > dataFrames[1].count() # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating and Appending Rows (Union) & Sorting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|  Alice| 28|     New York|\n",
      "|    Bob| 35|San Francisco|\n",
      "|Charlie| 22|  Los Angeles|\n",
      "+-------+---+-------------+\n",
      "\n",
      "DataFrame 2:\n",
      "+-----+---+-------+\n",
      "| name|age|   city|\n",
      "+-----+---+-------+\n",
      "|David| 30|Chicago|\n",
      "|  Eva| 40| Boston|\n",
      "|Frank| 25|Seattle|\n",
      "+-----+---+-------+\n",
      "\n",
      "Concatenated DataFrame:\n",
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|  Alice| 28|     New York|\n",
      "|    Bob| 35|San Francisco|\n",
      "|Charlie| 22|  Los Angeles|\n",
      "|  David| 30|      Chicago|\n",
      "|    Eva| 40|       Boston|\n",
      "|  Frank| 25|      Seattle|\n",
      "+-------+---+-------------+\n",
      "\n",
      "Sorted DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|Charlie| 22|  Los Angeles|\n",
      "|  Frank| 25|      Seattle|\n",
      "|  Alice| 28|     New York|\n",
      "|  David| 30|      Chicago|\n",
      "|    Bob| 35|San Francisco|\n",
      "|    Eva| 40|       Boston|\n",
      "+-------+---+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data1 = [(\"Alice\", 28, \"New York\"),\n",
    "         (\"Bob\", 35, \"San Francisco\"),\n",
    "         (\"Charlie\", 22, \"Los Angeles\")]\n",
    "\n",
    "data2 = [(\"David\", 30, \"Chicago\"),\n",
    "         (\"Eva\", 40, \"Boston\"),\n",
    "         (\"Frank\", 25, \"Seattle\")]\n",
    "\n",
    "# Define the schema\n",
    "schema = [\"name\", \"age\", \"city\"]\n",
    "\n",
    "# Create DataFrames\n",
    "df1 = spark.createDataFrame(data1, schema=schema)\n",
    "df2 = spark.createDataFrame(data2, schema=schema)\n",
    "\n",
    "# Show the original DataFrames\n",
    "print(\"DataFrame 1:\")\n",
    "df1.show()\n",
    "\n",
    "print(\"DataFrame 2:\")\n",
    "df2.show()\n",
    "\n",
    "# Concatenate or append rows using union\n",
    "concatenated_df = df1.union(df2)\n",
    "\n",
    "# Show the concatenated DataFrame\n",
    "print(\"Concatenated DataFrame:\")\n",
    "concatenated_df.show()\n",
    "\n",
    "# Sort rows by the \"age\" column in ascending order\n",
    "sorted_df = concatenated_df.orderBy(\"age\")\n",
    "\n",
    "# Show the sorted DataFrame\n",
    "print(\"Sorted DataFrame:\")\n",
    "sorted_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limit\n",
    "Oftentimes, you might want to restrict what you extract from a DataFrame; for example, you\n",
    "might want just the top ten of some DataFrame. You can do this by using the limit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|  Alice| 28|     New York|\n",
      "|    Bob| 35|San Francisco|\n",
      "|Charlie| 22|  Los Angeles|\n",
      "+-------+---+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition and Coalesce\n",
    "Another important optimization opportunity is to partition the data according to some frequently\n",
    "filtered columns, which control the physical layout of data across the cluster including the\n",
    "partitioning scheme and the number of partitions.\n",
    "Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This\n",
    "means that you should typically only repartition when the future number of partitions is greater\n",
    "than your current number of partitions or when you are looking to partition by a set of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, city: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Rows to the Driver\n",
    "As discussed in previous chapters, Spark maintains the state of the cluster in the driver. There are\n",
    "times when you’ll want to collect some of your data to the driver in order to manipulate it on\n",
    "your local machine.\n",
    "Thus far, we did not explicitly define this operation. However, we used several different methods\n",
    "for doing so that are effectively all the same. collect gets all data from the entire DataFrame,\n",
    "take selects the first N rows, and show prints out a number of rows nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|  Alice| 28|     New York|\n",
      "|    Bob| 35|San Francisco|\n",
      "|Charlie| 22|  Los Angeles|\n",
      "+-------+---+-------------+\n",
      "\n",
      "+-------+---+-------------+\n",
      "|name   |age|city         |\n",
      "+-------+---+-------------+\n",
      "|Alice  |28 |New York     |\n",
      "|Bob    |35 |San Francisco|\n",
      "|Charlie|22 |Los Angeles  |\n",
      "+-------+---+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=28, city='New York'),\n",
       " Row(name='Bob', age=35, city='San Francisco'),\n",
       " Row(name='Charlie', age=22, city='Los Angeles')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x7f70042f0ba0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Thank You!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
